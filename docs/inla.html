<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 INLA | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</title>
  <meta name="description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 INLA | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />
  <meta property="og:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 INLA | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  
  <meta name="twitter:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />

<meta name="author" content="Candidate: Niccolò Salvini" />
<meta name="author" content="Supervisor: PhD Marco L. Della Vedova" />
<meta name="author" content="Assistant Supervisor: PhD Vincenzo Nardelli" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/logo/spatial_logo.png" />
  <link rel="shortcut icon" href="images/logo/spatial_logo.ico" type="image/x-icon" />
<link rel="prev" href="Infrastructure.html"/>
<link rel="next" href="prdm.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.16/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.3/leaflet.js"></script>
<link href="libs/leaflet-easybutton-1.3.1/easy-button.css" rel="stylesheet" />
<script src="libs/leaflet-easybutton-1.3.1/easy-button.js"></script>
<script src="libs/leaflet-easybutton-1.3.1/EasyButton-binding.js"></script>
<link href="libs/leaflet-locationfilter2-0.1.1/locationfilter.css" rel="stylesheet" />
<script src="libs/leaflet-locationfilter2-0.1.1/locationfilter.js"></script>
<script src="libs/leaflet-locationfilter2-0.1.1/locationfilter-bindings.js"></script>
<link href="libs/ionicons-2.0.1/ionicons.min.css" rel="stylesheet" />
<link href="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.css" rel="stylesheet" />
<script src="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.js"></script>
<script src="libs/leaflet-minimap-3.3.1/Minimap-binding.js"></script>
<script src="libs/core-js-2.5.3/shim.min.js"></script>
<script src="libs/react-16.12.0/react.min.js"></script>
<script src="libs/react-16.12.0/react-dom.min.js"></script>
<script src="libs/reactwidget-1.0.0/react-tools.js"></script>
<script src="libs/reactable-binding-0.2.3/reactable.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>
<script src="js/1book.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logo/spatial_logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i>Colophon</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Web Scraping</a>
<ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#reverse"><i class="fa fa-check"></i><b>2.1</b> A Gentle Introduction on Web Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#anatomy-of-a-url-and-reverse-engineering"><i class="fa fa-check"></i><b>2.2</b> Anatomy of a url and reverse engineering</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="scraping.html"><a href="scraping.html#scraping-with-rvest"><i class="fa fa-check"></i><b>2.2.1</b> Scraping with <code id="ContentArchitecture">rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#ProperScraping"><i class="fa fa-check"></i><b>2.3</b> Searching Technique for Scraping</a></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.4</b> Scraping Best Practices and Security provisions</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#HTTPmethod"><i class="fa fa-check"></i><b>2.5</b> HTTP overview</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.5.1</b> User Agent and further Identification Headers Spoofing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#possibly"><i class="fa fa-check"></i><b>2.6</b> Dealing with failure</a></li>
<li class="chapter" data-level="2.7" data-path="scraping.html"><a href="scraping.html#parallelscraping"><i class="fa fa-check"></i><b>2.7</b> Parallel Scraping</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="scraping.html"><a href="scraping.html#parallel-furrrfuture"><i class="fa fa-check"></i><b>2.7.1</b> Parallel furrr+future</a></li>
<li class="chapter" data-level="2.7.2" data-path="scraping.html"><a href="scraping.html#parallel-foreachdofuture"><i class="fa fa-check"></i><b>2.7.2</b> Parallel foreach+doFuture</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="scraping.html"><a href="scraping.html#legal"><i class="fa fa-check"></i><b>2.8</b> Legal Profiles</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> API Technology Stack</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#restful-api"><i class="fa fa-check"></i><b>3.1</b> RESTful API</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.1.1</b> Plumber HTTP API</a></li>
<li class="chapter" data-level="3.1.2" data-path="Infrastructure.html"><a href="Infrastructure.html#sanitize"><i class="fa fa-check"></i><b>3.1.2</b> Sanitization</a></li>
<li class="chapter" data-level="3.1.3" data-path="Infrastructure.html"><a href="Infrastructure.html#DoS"><i class="fa fa-check"></i><b>3.1.3</b> Denial Of Service (DoS)</a></li>
<li class="chapter" data-level="3.1.4" data-path="Infrastructure.html"><a href="Infrastructure.html#logging"><i class="fa fa-check"></i><b>3.1.4</b> Logging</a></li>
<li class="chapter" data-level="3.1.5" data-path="Infrastructure.html"><a href="Infrastructure.html#docs"><i class="fa fa-check"></i><b>3.1.5</b> RESTful API docs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.2</b> Docker</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.1</b> REST-API container</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.3</b> NGINX reverse Proxy Server and Authorization</a></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-compose"><i class="fa fa-check"></i><b>3.4</b> Docker-Compose</a></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#HTTPS"><i class="fa fa-check"></i><b>3.5</b> HTTPS(ecure) and SSL certificates</a></li>
<li class="chapter" data-level="3.6" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.6</b> AWS EC2 instance</a></li>
<li class="chapter" data-level="3.7" data-path="Infrastructure.html"><a href="Infrastructure.html#sdwf"><i class="fa fa-check"></i><b>3.7</b> Software CI/CD Workflow</a></li>
<li class="chapter" data-level="3.8" data-path="Infrastructure.html"><a href="Infrastructure.html#further-sf-integrations"><i class="fa fa-check"></i><b>3.8</b> Further SF Integrations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inla.html"><a href="inla.html"><i class="fa fa-check"></i><b>4</b> INLA</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inla.html"><a href="inla.html#LGM"><i class="fa fa-check"></i><b>4.1</b> The class of Latent Gaussian Models (LGM)</a></li>
<li class="chapter" data-level="4.2" data-path="inla.html"><a href="inla.html#gmrf"><i class="fa fa-check"></i><b>4.2</b> Gaussian Markov Random Field (GMRF)</a></li>
<li class="chapter" data-level="4.3" data-path="inla.html"><a href="inla.html#approx"><i class="fa fa-check"></i><b>4.3</b> INLA Laplace Approximations</a></li>
<li class="chapter" data-level="4.4" data-path="inla.html"><a href="inla.html#rinla"><i class="fa fa-check"></i><b>4.4</b> R-INLA package</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>5</b> Geostatistical Data Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>5.1</b> Gaussian Process (GP)</a></li>
<li class="chapter" data-level="5.2" data-path="prdm.html"><a href="prdm.html#spdeapproach"><i class="fa fa-check"></i><b>5.2</b> The Stochastic Partial Differential Equation (SPDE) approach</a></li>
<li class="chapter" data-level="5.3" data-path="prdm.html"><a href="prdm.html#hedonic-rental-price-models"><i class="fa fa-check"></i><b>5.3</b> Hedonic (rental) Price Models</a></li>
<li class="chapter" data-level="5.4" data-path="prdm.html"><a href="prdm.html#criticism"><i class="fa fa-check"></i><b>5.4</b> Model Criticism</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="prdm.html"><a href="prdm.html#predbase"><i class="fa fa-check"></i><b>5.4.1</b> Methods based on the predictive distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="prdm.html"><a href="prdm.html#devbased"><i class="fa fa-check"></i><b>5.4.2</b> Deviance-based Criteria</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="prdm.html"><a href="prdm.html#priorsspec"><i class="fa fa-check"></i><b>5.5</b> Penalized Complexity Priors</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>6</b> Exploratory Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="exploratory.html"><a href="exploratory.html#prep"><i class="fa fa-check"></i><b>6.1</b> Preprocessing and Feature Engineering</a></li>
<li class="chapter" data-level="6.2" data-path="exploratory.html"><a href="exploratory.html#spatassess"><i class="fa fa-check"></i><b>6.2</b> Spatial Dependece Assessement</a></li>
<li class="chapter" data-level="6.3" data-path="exploratory.html"><a href="exploratory.html#factor-counts"><i class="fa fa-check"></i><b>6.3</b> Factor Counts</a></li>
<li class="chapter" data-level="6.4" data-path="exploratory.html"><a href="exploratory.html#mvp"><i class="fa fa-check"></i><b>6.4</b> Assessing the most valuable properties</a></li>
<li class="chapter" data-level="6.5" data-path="exploratory.html"><a href="exploratory.html#assessing-relevant-predictors"><i class="fa fa-check"></i><b>6.5</b> Assessing relevant predictors</a></li>
<li class="chapter" data-level="6.6" data-path="exploratory.html"><a href="exploratory.html#missassimp"><i class="fa fa-check"></i><b>6.6</b> Missing Assessement and Imputation</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="exploratory.html"><a href="exploratory.html#missing-assessement"><i class="fa fa-check"></i><b>6.6.1</b> Missing Assessement</a></li>
<li class="chapter" data-level="6.6.2" data-path="exploratory.html"><a href="exploratory.html#missing-imputation"><i class="fa fa-check"></i><b>6.6.2</b> Missing Imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modelspec.html"><a href="modelspec.html"><i class="fa fa-check"></i><b>7</b> Model Selection &amp; Fitting</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modelspec.html"><a href="modelspec.html#modelspecandmesh"><i class="fa fa-check"></i><b>7.1</b> Model Specification &amp; Mesh Assessement</a></li>
<li class="chapter" data-level="7.2" data-path="modelspec.html"><a href="modelspec.html#spdemodeol"><i class="fa fa-check"></i><b>7.2</b> Building the SPDE object</a></li>
<li class="chapter" data-level="7.3" data-path="modelspec.html"><a href="modelspec.html#model-selection"><i class="fa fa-check"></i><b>7.3</b> Model Selection</a></li>
<li class="chapter" data-level="7.4" data-path="modelspec.html"><a href="modelspec.html#fit"><i class="fa fa-check"></i><b>7.4</b> Parameter Estimation and Results</a></li>
<li class="chapter" data-level="7.5" data-path="modelspec.html"><a href="modelspec.html#plot-gmrf"><i class="fa fa-check"></i><b>7.5</b> Plot GMRF</a></li>
<li class="chapter" data-level="7.6" data-path="modelspec.html"><a href="modelspec.html#spatial-model-criticism"><i class="fa fa-check"></i><b>7.6</b> Spatial Model Criticism</a></li>
<li class="chapter" data-level="7.7" data-path="modelspec.html"><a href="modelspec.html#spatial-prediction-on-a-grid"><i class="fa fa-check"></i><b>7.7</b> Spatial Prediction on a Grid</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>8</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="8.1" data-path="appendix.html"><a href="appendix.html#triangular"><i class="fa fa-check"></i><b>8.1</b> SPDE and Triangulation</a></li>
<li class="chapter" data-level="8.2" data-path="appendix.html"><a href="appendix.html#laplaceapprox"><i class="fa fa-check"></i><b>8.2</b> Laplace Approximation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inla" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> INLA</h1>
<p>Bayesian estimation methods - by MCMC <span class="citation">(<a href="references.html#ref-mcmc" role="doc-biblioref">Brooks et al. 2011</a>)</span> and MC simulation techniques - are usually much harder than Frequentist calculations <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">Wang, Yue, and Faraway 2018</a>)</span>. This unfortunately is also more critical for spatial and spatio-temporal model settings <span class="citation">(<a href="references.html#ref-Cameletti2012" role="doc-biblioref">Cameletti et al. 2012</a>)</span> where matrices dimensions and densities (in the sense of prevalence of values throughout the matrix) start becoming unfeasible.
The computational aspect refers in particular to the ineffectiveness of linear algebra operations with large dense covariance matrices that in aforementioned settings scale to the order <span class="math inline">\(\mathcal{O}(n^3)\)</span>.
INLA <span class="citation">(<a href="references.html#ref-Rue2009" role="doc-biblioref">Håvard Rue, Martino, and Chopin 2009</a>; <a href="references.html#ref-Rue2017" role="doc-biblioref">Håvard Rue et al. 2017</a>)</span> stands for Integrated Nested Laplace Approximation and constitutes a faster and accurate deterministic algorithm whose performance in time indeed scale to the order <span class="math inline">\(\mathcal{O}(n)\)</span>. INLA is alternative and by no means substitute <span class="citation">(<a href="references.html#ref-YT:Rue" role="doc-biblioref">Rencontres Mathématiques 2018</a>)</span> to traditional Bayesian Inference methods. INLA focuses on Latent Gaussian Models LGM <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span>, which are a rich class including many regressions models, as well as support for spatial and spatio-temporal.
INLA turns out to shorten model fitting time for essentially two reasons related to clever LGM model settings, such as: Gaussian Markov random field (GMRF) offering sparse matrices representation and Laplace approximation to approximate posterior marginals’ integrals with proper search strategies.
In the end of the chapter it is presented the R-INLA project and the package focusing on the essential aspects.
Further notes: the chronological steps followed in the methodology presentation retraces the canonical one by <span class="citation"><a href="references.html#ref-Rue2009" role="doc-biblioref">Håvard Rue, Martino, and Chopin</a> (<a href="references.html#ref-Rue2009" role="doc-biblioref">2009</a>)</span>, which according to the author’s opinion is the most effective and the default one for all the related literature. The approach is more suitable and remains quite unchanged since it is top-down, which naturally fits the hierarchical framework imposed in INLA. The GMRF theory section heavily relies on <span class="citation"><a href="references.html#ref-GMRFRue" role="doc-biblioref">Havard Rue and Held</a> (<a href="references.html#ref-GMRFRue" role="doc-biblioref">2005</a>)</span>.<br />
Notation is imported from <span class="citation"><a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">Michela Blangiardo Marta; Cameletti</a> (<a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> and integrated with <span class="citation"><a href="references.html#ref-Bayesian_INLA_Rubio" role="doc-biblioref">Gómez Rubio</a> (<a href="references.html#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>, whereas examples are drawn from <span class="citation"><a href="references.html#ref-wang2018bayesian" role="doc-biblioref">Wang, Yue, and Faraway</a> (<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span>. Vectors and matrices are typeset in bold i.e. <span class="math inline">\(\boldsymbol{\beta}\)</span>, so each time they occur they have to be considered such as the <em>ensamble</em> of their values, whereas the notation <span class="math inline">\(\beta_{-i}\)</span> denotes all elements in <span class="math inline">\(\boldsymbol{\beta}\)</span> but <span class="math inline">\(\beta_{-i}\)</span>.
<span class="math inline">\(\pi(\cdot)\)</span> is a generic notation for the density of its arguments and <span class="math inline">\(\tilde\pi(\cdot)\)</span> has to be intended as its Laplace approximation. Furthermore Laplace Approximations mathematical details, e.g. optimal grid strategies and integration points, are overlooked instead a quick intuition on Laplace functioning is offered in the appendix <a href="appendix.html#laplaceapprox">8.2</a>.</p>
<div id="LGM" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> The class of Latent Gaussian Models (LGM)</h2>
<p>Bayesian theory is straightforward, but it is not always simple to measure posterior and other quantities of interest <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">Wang, Yue, and Faraway 2018</a>)</span>. There are three ways to obtain a posterior estimate: by <em>Exact</em> estimation, i.e. operating on conjugate priors, but there are relatively few conjugate priors which are also employed in simple models. By <em>Sampling</em> through generating samples from the posterior distributions with MCMC methods <span class="citation">(<a href="references.html#ref-metropolis" role="doc-biblioref">Metropolis et al. 1953</a>; <a href="references.html#ref-hastings" role="doc-biblioref">Hastings 1970</a>)</span> later applied in Bayesian statistics by <span class="citation"><a href="references.html#ref-gelfand1990sampling" role="doc-biblioref">Gelfand and Smith</a> (<a href="references.html#ref-gelfand1990sampling" role="doc-biblioref">1990</a>)</span>. MCMCs have improved over time due to inner algorithm optimization as well as both hardware and software progresses, nevertheless for certain model combinations and data they either do not converge or take an unacceptable amount of time <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span>. By <em>Approximation</em> through numerical integration and INLA can count on a strategy leveraging on three elements: <em>LGMs</em>, <em>Gaussian Markov Random Fields</em> (GMRF) and <em>Laplace Approximations</em> and this will articulates the steps according to which the arguments are treated. LGMs despite their anonymity are very flexible and they can host a wide range of models as regression, dynamic, spatial, spatio-temporal <span class="citation">(<a href="references.html#ref-Cameletti2012" role="doc-biblioref">Cameletti et al. 2012</a>)</span>. LGMs necessitate further three interconnected elements: <strong>Likelihood</strong>, <strong>Latent field</strong> and <strong>Priors</strong>.
To start it can be specified a generalization of a linear predictor <span class="math inline">\(\eta_{i}\)</span> which takes into account both linear and non-linear effects on covariates:</p>
<p><span class="math display" id="eq:linearpredictor">\[\begin{equation}
  \eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)
\tag{4.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the intercept, <span class="math inline">\(\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}\)</span> are the coefficients that quantifies the linear effects of covariates <span class="math inline">\(\boldsymbol{x}=\left({x}_{1}, \ldots, {x}_{M}\right)\)</span> and <span class="math inline">\(f_{l}(\cdot), \forall l \in 1 \ldots L\)</span> are a set of random effects defined in terms of a <span class="math inline">\(\boldsymbol{z}\)</span> set of covariates <span class="math inline">\(\boldsymbol{z}=\left(z_{1}, \ldots, z_{L}\right)\)</span> e.g. Random Walks <span class="citation">(<a href="references.html#ref-GMRFRue" role="doc-biblioref">Havard Rue and Held 2005</a>)</span>, Gaussian Processes <span class="citation">(<a href="references.html#ref-besag1995conditional" role="doc-biblioref">Besag and Kooperberg 1995</a>)</span>, such models are termed as General Additive Models i.e. GAM or Generalized Linear Mixed Models GLMM <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">Wang, Yue, and Faraway 2018</a>)</span>.
For the response <span class="math inline">\(\mathbf{y}=\left(y_{1}, \ldots, y_{n}\right)\)</span> it is specified an <em>exponential family</em> distribution function whose mean <span class="math inline">\(\mu_i\)</span> (computed as its expectation <span class="math inline">\(\left.E\left(\mathbf{y}\right)\right)\)</span>) is linked via a link function <span class="math inline">\(\mathscr{g}(\cdot)\)</span> to <span class="math inline">\(\eta_{i}\)</span> in eq. <a href="inla.html#eq:linearpredictor">(4.1)</a>, i.e. <span class="math inline">\(\mathscr{g}\left(\mu_i\right)=\eta_{i}\)</span>. At this point is possible to group all the latent (in the sense of unobserved) inference components into a variable, said <strong>latent field</strong> and denoted as <span class="math inline">\(\boldsymbol{\theta}\)</span> such that: <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, f\right\}\)</span>, where each single observation <span class="math inline">\(i\)</span> is connected to a <span class="math inline">\(\theta_{i}\)</span> combination of parameters in <span class="math inline">\(\boldsymbol{\theta}\)</span>.
The latent parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> actually may depend on some hyper-parameters <span class="math inline">\(\boldsymbol{\psi} = \left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span>. Then, given <span class="math inline">\(\mathbf{y}\)</span>, the joint probability distribution function conditioned to both parameters and hyper-parameters, assuming <em>conditional independence</em>, is expressed by the <strong>likelihood</strong>:</p>
<p><span class="math display" id="eq:jpd">\[\begin{equation}
  \pi(\boldsymbol{\mathbf{y}} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i\ = 1}^{\mathbf{I}} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)
\tag{4.2}
\end{equation}\]</span></p>
<p>The conditional independence assumption grants that for a general couple of conditionally independent <span class="math inline">\(\theta_j\)</span> and <span class="math inline">\(\theta_i\)</span>, where <span class="math inline">\(i \neq j\)</span>, the joint conditional distribution is factorized by <span class="math inline">\(\pi\left(\theta_{i}, \theta_{j} \mid \theta_{-i, j}\right)=\pi\left(\theta_{i} \mid \theta_{-i, j}\right) \pi\left(\theta_{j} \mid \theta_{-i, j}\right)\)</span> <span class="citation">(<a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">Michela Blangiardo Marta; Cameletti 2015</a>)</span>, i.e. the likelihood in eq:<a href="inla.html#eq:jpd">(4.2)</a>. The assumption constitutes a building block in INLA since as it will be shown later it will assure that there will be 0 patterns encoded inside matrices, implying computational benefits.
<!-- The conditional independence assumption [@GMRFRue] (i.e. $\pi\left(\theta_{i}, \theta_{j} \mid \theta_{-i, j}\right)=\pi\left(\theta_{i} \mid \theta_{-i, j}\right) \pi\left(\theta_{j} \mid \theta_{-i, j}\right)$), states that the joint conditional distribution is given by the product of all the independent parameters i.e. the likelihood. -->
Note also that the product index <span class="math inline">\(i\)</span> ranges from 1 to <span class="math inline">\(\mathbf{I}\)</span>, i.e. <span class="math inline">\(\mathbf{I} = \left\{1 \ldots n \right\}\)</span>. In the case when an observations are missing, i.e. <span class="math inline">\(i \notin \mathbf{I}\)</span>, INLA automatically discards missing values from the model estimation <span class="citation">(<a href="references.html#ref-Bayesian_INLA_Rubio" role="doc-biblioref">2020</a>)</span>, this would be critical during missing values imputation sec. <a href="exploratory.html#missassimp">6.6</a>.
At this point, as required by LGM, are needed to be imposed <em>Gaussian priors</em> on each linear effect and each model covariate that have either a univariate or <em>multivaried normal</em> density in order to make the additive <span class="math inline">\(\eta_i\)</span> Gaussian <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span>.
An example might clear up the setting requirement: let us assume to have a Normally distributed response and let us set the goal to specify a Bayesian Generalized Linear Model (GLM). Then the linear predictor can have this appearance <span class="math inline">\(\eta_{i}=\beta_{0}+\beta_{1} x_{i 1}, \quad i=1, \ldots, n\)</span>, where <span class="math inline">\(\beta_{0}\)</span> is the intercept and <span class="math inline">\(\beta_{1}\)</span> is the slope for a general covariate <span class="math inline">\(x_{i1}\)</span>. While applying LGM are <strong>needed</strong> to be specified Gaussian priors on <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>, such that: <span class="math inline">\(\beta_{0} \sim N\left(\mu_{0}, \sigma_{0}^{2}\right)\)</span> and <span class="math inline">\(\beta_{1} \sim N\left(\mu_{1}, \sigma_{1}^{2}\right)\)</span>, for which the latent linear predictor <span class="math inline">\(\eta_i\)</span> is <span class="math inline">\(\eta_{i} \sim N\left(\mu_{0}+\mu_{1} x_{i 1}, \sigma_{0}^{2}+\sigma_{1}^{2} x_{i 1}^{2}\right)\)</span>. It can be illustrated by some linear algebra <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span> that <span class="math inline">\(\boldsymbol{\eta}=\left(\eta_{1}, \ldots, \eta_{n}\right)^{\prime}\)</span> is a Gaussian Process with mean structure <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{Q^{-1}}\)</span>. The hyperparameters <span class="math inline">\(\sigma_{0}^{2}\)</span> and
<span class="math inline">\(\sigma_{1}^{2}\)</span> are to be either fixed or estimated by taking hyperpriors on them. In this context <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \beta_{1}\right\}\)</span> can group all the latent components and <span class="math inline">\(\boldsymbol{\psi} = \left\{\sigma_{0}^{2},\sigma_{1}^{2}\right\}\)</span> is the vector of <strong>Priors</strong>.
<!-- Moreover there exists a special class of LG, namely Gaussian Markov random field (GMRF), which is very effective and versatile in modeling paricular effects in LGMs. -->
For what it can be noticed there is a clear hierarchical relationship for which three different levels are seen: a <em>higher</em> level represented by the exponential family distribution function on <span class="math inline">\(\mathbf{y}\)</span>, given the latent parameter and the hyper parameters. The <em>medium</em> by latent Gaussian random field with density function given some other hyper parameters. The <em>lower</em> by the joint distribution or a product of several distributions for which priors can be specified
So letting be <span class="math inline">\(\boldsymbol{\mathbf{y}}=\left(y_{1}, \ldots, y_{n}\right)^{\prime}\)</span> at the <em>higher</em> level it is assumed an exponential family distribution function given a first set of hyper-parameters <span class="math inline">\(\boldsymbol\psi_1\)</span>, usually referred to measurement error precision <span class="citation"><a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">Michela Blangiardo Marta; Cameletti</a> (<a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span>). Therefore as in <a href="inla.html#eq:jpd">(4.2)</a>,</p>
<p><span class="math display" id="eq:higher">\[\begin{equation}
  \pi(\boldsymbol{\mathbf{y}} \mid \boldsymbol{\theta}, \boldsymbol{\psi_1})=\prod_{i\ = 1}^{\mathbf{I}} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi_1}\right)
\tag{4.3}
\end{equation}\]</span></p>
<p>At the <em>medium</em> level it is specified on the latent field <span class="math inline">\(\boldsymbol\theta\)</span> a latent Gaussian random field (LGRF), given <span class="math inline">\(\boldsymbol\psi_2\)</span> i.e. the rest of the hyper-parameters,</p>
<p><span class="math display" id="eq:medium">\[\begin{equation}
  \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi_2})=(2 \pi)^{-n / 2}| \boldsymbol{Q(\psi_2)}|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q(\psi_2)} \boldsymbol{\theta}\right)
\tag{4.4}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{Q(\psi_2)}\)</span> denotes positive definite matrix and <span class="math inline">\(|\cdot|\)</span> its determinant. <span class="math inline">\(\prime\)</span> is the transpose operator. The matrix <span class="math inline">\(\boldsymbol{Q(\psi_2)}\)</span> is called the <em>precision matrix</em> that outlines the underlying dependence structure of the data, and its inverse <span class="math inline">\(\boldsymbol{Q(\cdot)}^{-1}\)</span> is the covariance matrix <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">Wang, Yue, and Faraway 2018</a>)</span>. In the spatial setting this would be critical since by a specifying a multivariate Normal distribution of eq. <a href="inla.html#eq:medium">(4.4)</a> it will become a GMRF. Due to conditional independence GMRF precision matrices are sparse and through linear algebra and numerical method for sparse matrices model fitting time is saved <span class="citation">(<a href="references.html#ref-GMRFRue" role="doc-biblioref">Havard Rue and Held 2005</a>)</span>.
In the <em>lower</em> level priors are collected togheter <span class="math inline">\(\boldsymbol\psi_ =\{ \boldsymbol\psi_1\ , \boldsymbol\psi_2\}\)</span> for which are specified either a single prior distribution or a joint prior distribution as the product of its independent priors.
Since the end goal is to find the joint posterior for <span class="math inline">\(\boldsymbol\theta\)</span> and <span class="math inline">\(\boldsymbol\psi\)</span>, then given priors <span class="math inline">\(\boldsymbol\psi\)</span> it possible to combine expression <a href="inla.html#eq:higher">(4.3)</a> with <a href="inla.html#eq:medium">(4.4)</a> obtaining:</p>
<p><span class="math display" id="eq:formallgm">\[\begin{equation}
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \mathbf{y})\propto  \underbrace{\pi(\boldsymbol{\psi})}_{\text {priors}} \times \underbrace{\pi(\boldsymbol\theta \mid \boldsymbol\psi)}_{\text {LGRM}} \times \underbrace{\prod_{i=1}^{\mathbf{I}} \pi\left(\mathbf{y} \mid \boldsymbol\theta, \boldsymbol{\psi}\right)}_{\text {likelihood }}
\tag{4.5}
\end{equation}\]</span></p>
<p>Which can be further solved following <span class="citation">(<a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">Michela Blangiardo Marta; Cameletti 2015</a>)</span> as:</p>
<p><span class="math display" id="eq:finallgm">\[\begin{equation}
\begin{aligned}
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y) &amp; \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \pi(\mathbf{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) \\
&amp; \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\
&amp; \propto \pi(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}\right) \times \prod_{i}^{n} \exp \left(\log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right)
\end{aligned}
\tag{4.6}
\end{equation}\]</span></p>
<p>From which the two quantities of interest are the posterior marginal distribution for each element in the latent field and for each hyper parameter.</p>
<p><span class="math display" id="eq:finalobj">\[\begin{equation}
\begin{aligned}
\begin{array}{l}
\pi\left(\theta_{i} \mid \boldsymbol{\mathbf{y}}\right)=\int \pi\left(\boldsymbol{\theta}_{i} \mid \boldsymbol{\psi}, \boldsymbol{\mathbf{y}}\right) \pi(\boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}}) d \boldsymbol{\psi} \\
\pi\left(\boldsymbol{\psi}_{k} \mid \boldsymbol{\mathbf{y}}\right)=\int \pi(\boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}}) d \boldsymbol{\psi}_{-k}
\end{array}
\end{aligned}
\tag{4.7}
\end{equation}\]</span></p>
<p>From final eq: <a href="inla.html#eq:finallgm">(4.6)</a> is derived Bayesian inference and INLA through Laplace can approximate posterior parameters distributions. Sadly, INLA cannot effectively suit all LGM’s. In general INLA depends upon the following supplementary assumptions <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span>:</p>
<ul>
<li>The hyper-parameter number <span class="math inline">\(\boldsymbol\psi\)</span> should be unpretentious, normally between 2 and 5, but not greater than 20.</li>
<li>When the number of observation is considerably high (<span class="math inline">\(10^4\)</span> to <span class="math inline">\(10^5\)</span>), then the LGMR <span class="math inline">\(\boldsymbol\theta\)</span> must be a Gaussian Markov random field (GMRF).</li>
</ul>
</div>
<div id="gmrf" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Gaussian Markov Random Field (GMRF)</h2>
<p>In the order to make INLA working efficiently the latent field <span class="math inline">\(\boldsymbol\theta\)</span> must not only be Gaussian but also Gaussian Markov Random Field (from now on GMRF). A GMRF is a genuinely simple structure: It is just random vector following a multivariate normal (or Gaussian) distribution <span class="citation">(<a href="references.html#ref-GMRFRue" role="doc-biblioref">Havard Rue and Held 2005</a>)</span>. However It is more interesting to research a restricted set of GMRF for which are satisfied the conditional independence assumptions (section <a href="inla.html#LGM">4.1</a>), from here the term “Markov.” Expanding the concept of conditional independece let us assume to have a vector <span class="math inline">\(\boldsymbol{\mathbf{x}}=\left(x_{1}, x_{2}, x_{3}\right)^{T}\)</span> where <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are conditionally independent given <span class="math inline">\(x_3\)</span>, i.e. <span class="math inline">\(x_{1} \perp x_{2} \mid x_3\)</span>. With that said if the objective is <span class="math inline">\(x_3\)</span>, then uncovering <span class="math inline">\(x_2\)</span> gives no information on <span class="math inline">\(x_1\)</span>. The joint density for <span class="math inline">\(\boldsymbol{\mathbf{x}}\)</span> is</p>
<p><span class="math display" id="eq:pix">\[\begin{equation}
  \pi(\boldsymbol{\mathbf{x}})=\pi\left(x_{1} \mid x_{3}\right) \pi\left(x_{2} \mid x_{3}\right) \pi\left(x_{3}\right)
\tag{4.8}
\end{equation}\]</span></p>
<p>Now let us assume a more general case of AR(1) exploiting the possibilities of defining <span class="math inline">\(f_{1}(\cdot)\)</span> function through the eq. <a href="inla.html#eq:linearpredictor">(4.1)</a>. AR(1) is an <em>autoregressive model</em> of order 1 specified on the latent linear predictor <span class="math inline">\(\boldsymbol\eta\)</span> (notation slightly changes using <span class="math inline">\(\eta\)</span> instead of <span class="math inline">\(\theta\)</span> since latent components are few), with constant variance <span class="math inline">\(\sigma_{\eta}^{2}\)</span> and standard normal errors <span class="citation">(<a href="references.html#ref-GMRFRue" role="doc-biblioref">2005</a>; <a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span>. The model may have following expression:</p>
<p><span class="math display">\[
\eta_t=\phi \eta_{t-1}+\epsilon_{t}, \quad \epsilon_{t} \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(0,1), \quad|\phi|&lt;1
\]</span>
Where <span class="math inline">\(t\)</span> pedix is the time index and <span class="math inline">\(\phi\)</span> is the correlation in time. The conditional form of the previous equation can be rewritten when <span class="math inline">\(t = 2 \ldots n\)</span>:</p>
<p><span class="math display">\[
\eta_{t} \mid \eta_{1}, \ldots, \eta_{t-1} \sim \mathcal{N}\left(\phi \eta_{t-1}, \sigma_{\eta}^{2}\right)
\]</span>
Then let us also consider the marginal distribution for each <span class="math inline">\(\eta_i\)</span>, it can be proven to be Gaussian with mean 0 and variance <span class="math inline">\(\sigma_{\eta}^{2} /\left(1-\phi^{2}\right)\)</span> <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span>. Moreover the covariance between each general <span class="math inline">\(\eta_{i}\)</span> and <span class="math inline">\(\eta_{j}\)</span> is defined as <span class="math inline">\(\sigma_{\eta}^{2} \rho^{|i-j|} /\left(1-\rho^{2}\right)\)</span> which vanishes the more the distance <span class="math inline">\(|i-j|\)</span> increases.
Therefore <span class="math inline">\(\boldsymbol\eta\)</span> is a Gaussian Process, whose proper definition is in <a href="#def:gp"><strong>??</strong></a>, with mean structure of <em>0s</em> and covariance matrix <span class="math inline">\(\boldsymbol{Q}^{-1}\)</span> i.e. <span class="math inline">\(\boldsymbol{\eta} \sim N(\mathbf{0}, \boldsymbol{Q}^{-1})\)</span>. <span class="math inline">\(\boldsymbol{Q}^{-1}\)</span> is an <span class="math inline">\(n \times n\)</span> dense matrix that complicates computations.
But by a simple trick it is possible to recognize that AR(1) is a special type of GP with sparce precision matrix which is evident by showing the joint distribution for <span class="math inline">\(\boldsymbol\eta\)</span></p>
<p><span class="math display">\[
\pi(\boldsymbol{\eta})=\pi\left(\eta_{1}\right) \pi\left(\eta_{2} \mid \eta_{1}\right) \pi\left(\eta_{3} \mid \eta_{1}, \eta_{2}\right) \cdots \pi\left(\eta_{n} \mid \eta_{n-1}, \ldots, \eta_{1}\right)
\]</span>
whose precision matrix compared to its respective covariance matrix is:</p>
<div class="figure"><span id="fig:precisionmat"></span>
<img src="images/precvscov.jpg" alt="" />
<p class="caption">Figure 4.1: Precision Matrix in GMRF vs the Covariance matrix, source <span class="citation"><a href="references.html#ref-GMRFRue" role="doc-biblioref">Havard Rue and Held</a> (<a href="references.html#ref-GMRFRue" role="doc-biblioref">2005</a>)</span></p>
</div>
<p>with zero entries outside the diagonal (right panel fig. <a href="inla.html#fig:precisionmat">4.1</a>) and first off-diagonals <span class="citation">(<a href="references.html#ref-GMRFRue" role="doc-biblioref">2005</a>)</span>.
The conditional independence assumption makes the precision matrix tridiagonal since for generals <span class="math inline">\(\eta_i\)</span> and <span class="math inline">\(\eta_j\)</span> are conditionally independent for <span class="math inline">\(|i − j| &gt; 1\)</span>, given all the rest. In other words <span class="math inline">\(\boldsymbol{Q}\)</span> is sparse since given all the latent predictors in <span class="math inline">\(\boldsymbol\eta\)</span>, then <span class="math inline">\(\eta_t\)</span> depends only on the preceding <span class="math inline">\(\eta_{t-1}\)</span>. For example in <a href="inla.html#eq:conditinal">(4.9)</a>, let assume to have <span class="math inline">\(\eta_2\)</span> and <span class="math inline">\(\eta_4\)</span>, then:</p>
<p><span class="math display" id="eq:conditinal">\[\begin{equation} 
\begin{split}
  \pi\left(\eta_{2}, \eta_{4} \mid \eta_{1}, \eta_{3}\right) &amp;=\pi\left(\eta_{2} \mid \eta_{1}\right) \pi\left(\eta_{4} \mid \eta_{1}, \eta_{2}, \eta_{3}\right) \\
  &amp; =\pi\left(\eta_{2} \mid \eta_{1}\right) \pi\left(\eta_{4} \mid \eta_{3}\right)
\end{split}
\tag{4.9}
\end{equation}\]</span></p>
<p>For which the conditional density of <span class="math inline">\(\eta_2\)</span> does only depend on its preceding term i.e. <span class="math inline">\(\eta_1\)</span>. The same inner reasoning can be done for <span class="math inline">\(\eta_4\)</span>, which strictly depends on <span class="math inline">\(\eta_3\)</span> and vice versa.
Therefore ultimately it is possible to produce a rather formal definition of a GMRF:</p>

<div class="definition">
<span id="def:gmrf" class="definition"><strong>Definition 4.1  (GMRF)  </strong></span>A latent gaussian random field (LGRM) e.g. <span class="math inline">\(\boldsymbol\eta = \boldsymbol\theta\)</span> (when <span class="math inline">\(\mathscr{g}(\cdot)\)</span> is identity) is said a GMRF if it has a multivariate Normal density with additional conditional independence (also called the “Markov property”) <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">Wang, Yue, and Faraway 2018</a>)</span>.
</div>
<p><span class="math display">\[
\pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}_2) = \operatorname{MVN}(0, \boldsymbol{Q(\psi_2}))
\]</span></p>
<!-- <!-- PROVA DA QUA  -->
<!-- To start, for some observations $\mathbf{y}=\left(y_{1}, \ldots, y_{n}\right)$, it is convenient to specify an _exponential family_ distribution function which is characterized by some parameters $\phi_{i}$ (usually expressed by the mean $\left.E\left(y_{i}\right)\right)) and other hyper-parameters $\psi_{k} ,\forall k \in \ 1\ldots K$ [-@Blangiarod-Cameletti]. The parameters $\phi_{i}$ can be defined as an additive _latent linear predictor_ $\eta_{i}$ [@Krainski-Rubio], through a _link function_ $g(\cdot)$, i.e. $g\left(\phi_{i}\right)=\eta_{i}$. A generalization of the linear predictor takes into account both linear and non-linear effects on covariates: -->
<!-- Then it has to be specified an associated likelihood and many options are available [@Bayesian_INLA_Rubio; wang2018bayesian], the default choice is _Gaussian model_. -->
<!-- $$ -->
<!-- \eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right) -->
<!-- $$ -->
<!-- where $\beta_{0}$ is the intercept, $\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}$ are the coefficients that quantifies the linear effects of covariates $\boldsymbol{x}=\left({x}_{1}, \ldots, {x}_{M}\right)$ and $f_{l}(\cdot), \forall l \in 1 \ldots L$ are a set of random effects defined in terms of a $\boldsymbol{z}$ set of covariates $\boldsymbol{z}=\left(z_{1}, \ldots, z_{L}\right)$ (e.g. rw, ar1) [@Blangiardo-Cameletti]. As a consequence of the extended possibilities of combining mixed effects into LGMs, they contain a wide range of models e.g. GLM, GAM, GLMM, linear models and spatio-temporal models. This constitutes one of the main advantages of INLA algorithm, since it can fit many different models and integrate older ones with newer parameters. Furthermore INLA contributors recently are extending the methodology to many different areas of application and integrating the LG class with many other random effects _miss lit Martins et al., 2013 _. -->
<!-- With that said all the latent field components can be grouped into a variable denoted with $\boldsymbol{\theta}$ such that: $\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, f\right\}$ whose distribution depends on the hyper-parameter $\boldsymbol{\psi}$. The analogue can be repeated for hyper-parameters obtaining $\boldsymbol{\psi} = \left\{\psi_{1}, \ldots, \psi_{K}\right\}$.  -->
<!-- Then the probability distribution function conditioned to both parameters and hyper parameters is: -->
<!-- $$ -->
<!-- y_{i} \mid \boldsymbol{\theta}, \boldsymbol{\psi} \sim \pi\left(y_{i} \mid \boldsymbol{\theta},\boldsymbol{\psi}\right) -->
<!-- $$ -->
<!-- Since data $\left(y_{1}, \ldots, y_{n}\right)$ is drawn by the same distribution family but it is also conditioned to parameters which are said _conditional independent_ [@GMRFRue] (i.e. $\pi\left(\theta_{i}, \theta_{j} \mid \theta_{-i, j}\right)=\pi\left(\theta_{i} \mid \theta_{-i, j}\right) \pi\left(\theta_{j} \mid \theta_{-i, j}\right)$) , then the joint distribution is given by the product of all the independent parameters i.e. the likelihood. Note that the product index $i$ ranges from 1 to $n$, i.e.  $\mathbf{I} = \left\{1 \ldots n \right\}$. -->
<!-- In the case when an observation is missing, i.e. $i \notin \mathbf{I}$, INLA automatically discards missing values from the model estimation -@Bayesian_INLA_Rubio, this would be critical during missing values imputation \@ref(missassimp). -->
<!-- The likelihood expression is, where $\boldsymbol\theta^{\prime}$ is the transposed version of $\boldsymbol\theta$ and the $|\cdot|$ is the determinant: -->
<!-- \begin{equation} -->
<!-- \pi(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i \in \mathbb{I}} \pi\left(y_{i} \mid \theta^{\prime}_{i}, \boldsymbol{\psi}\right) -->
<!-- (\#eq:likelihood) -->
<!-- \end{equation} -->
<!-- Each data point is connected to a single combination $\theta_{i}$ in the $\boldsymbol{\theta}$ _latent field_. In fact the latent aspect of the field regards the undergoing existence of many parameter combination. Furthermore hyper-parameters are by definition independent, in other words $\boldsymbol{\psi}$ is the product of many univariate priors [@Bayesian_INLA_Rubio]. A Multivariate Normal distribution prior is imposed on the latent field $\boldsymbol{\theta}$ such that it is centered in 0 with precision matrix $\boldsymbol{Q(\psi)}$ (the inverse of the covariance matrix $\boldsymbol{Q}^{-1}(\boldsymbol{\psi})$)  depending only on $\boldsymbol{\psi}$ hyper-parameter vector i.e., $\boldsymbol{\theta} \sim \operatorname{Normal}\left(\mathbf{0}, \boldsymbol{Q}^{-1}(\boldsymbol{\psi})\right)$. As a notation remark some authors choose to keep the covariance matrix expression as $\boldsymbol{Q}$ and its inverse precision matrix as $\boldsymbol{Q}^{-1}$, equation \@ref(eq:gmrf). Using the covariance instead of precision is strongly not encouraged essentially for two reasons: the first is a practical one and regards the default hyper-parameter argument option in R INLA library, which adopts precision matrix notation. While the second accounts [@GMRFRue] the relationship between conditional independence and the -->
<!-- zero structure of the precision matrix, left in figure \@ref(fig:precvscov), that is dense in the covariance matrix, right in figure \@ref(fig:precvscov) notation case. -->
<!-- ![Precision Matrix in GMRF vs the Covariance matrix, source @GMRFRue](images/precvscov.jpg) -->
<!-- The exponential family density function can be rewritten as:  -->
<!-- \begin{equation} -->
<!-- \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi})=(2 \pi)^{-n / 2}| \boldsymbol{Q(\psi)}|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta} \boldsymbol{Q(\psi)} \boldsymbol{\theta}\right) -->
<!-- (\#eq:gmrf) -->
<!-- \end{equation} -->
<!-- The conditional independence assumption on each $\theta_i$ element of the latent field $\boldsymbol{\theta}$ leads $\boldsymbol{Q^{-1}(\psi)}$ to be a sparse precision matrix since for a general pair of combinations $\theta_{i}$ and $\theta_{j}$, when $ i \neq j$, the resulting element in the precision matrix is 0 i.e. $\theta_{i} \perp \theta_{j} \mid \theta_{-i, j} \Longleftrightarrow Q_{i j}(\boldsymbol{\psi})=0$ -@Blangiardo-Cameletti, where the notation $x_{-i}$ denotes all elements in $\boldsymbol{x}$ but $x_{-i}$.  -->
<!-- A probability distribution whose characteristics are the aforementioned is named  _Gaussian Markov random field_ (**GMRF**). GMRF as a matter of fact are Gaussian Variables with _Markov properties_ encoded in the precision matrix $\boldsymbol{Q}$ [@Rue2009]. The computational gain of doing inference with a GMRF is directly related to the sparse precision matrix $\boldsymbol{Q}$ structure. In fact, it can be done using numerical methods for the sparse matrices in linear algebras, leading to a substantial computational benefit [@Cameletti2012], [@GMRFRue], for the specific algorithms. -->
<!-- Once priors distributions are specified both for$\boldsymbol{\theta}$ and $\boldsymbol{\psi}$, then the joint posterior distribution is obtained by the product of the _GMRF_ \@ref(eq:gmrf) density, the _likelihood_ \@ref(eq:likelihood) and the hyper-parameter prior distribution: -->
<!-- $$ -->
<!-- \pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y)\propto  \underbrace{\pi(\boldsymbol{\psi})}_{\text {prior }} \times \underbrace{\pi(\theta \mid \psi)}_{\text {GMRF }} \times \underbrace{\prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)}_{\text {likelihood }} -->
<!-- $$ -->
<!-- Which can be further rewritten as in [@Blangiardo-Cameletti] as: -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y) & \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \pi(y \mid \boldsymbol{\theta}, \boldsymbol{\psi}) \\ -->
<!-- & \propto \pi(\boldsymbol{\psi}) \times \pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \prod_{i=1}^{n} \pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\ -->
<!-- & \propto \pi(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}\right) \times \prod_{i}^{n} \exp \left(\log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- In the end joining exponents by their multiplicative property -->
<!-- \begin{equation} -->
<!-- \pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid y) \propto \pi(\psi) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}+\sum^{n} \log \left(\pi\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) -->
<!-- (\#eq:jointpostdistr) -->
<!-- \end{equation} -->
</div>
<div id="approx" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> INLA Laplace Approximations</h2>
<p>The goals of the Bayesian inference are the marginal posterior distributions for each of the elements of the latent field. INLA is not going to try to approximate the whole joint posterior marginal distribution from expression <a href="inla.html#eq:finallgm">(4.6)</a> i.e. <span class="math inline">\(\pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{\mathbf{y}})\)</span>, in fact if it would (two-dimensional approx) it will cause a high biased approximations since it fail to capture both location and skewness in the marginals. Instead INLA algorithm will try to estimate the posterior marginal distribution for each <span class="math inline">\(\theta_{i}\)</span> in the latent parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>, for each hyper-parameter prior <span class="math inline">\(\psi_{k} \in \boldsymbol\psi\)</span> (back to the <span class="math inline">\(\boldsymbol\theta\)</span> latent field notation).</p>
<div class="bulb">
<p>The mathematical intuition behind <strong>Laplace Approximation</strong> along with some real life cases are contained in the appendix in sec. <a href="appendix.html#laplaceapprox">8.2</a>.</p>
</div>
<p>Therefore the key focus of INLA is to approximate with Laplace only densities that are near-Gaussian <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span> or replacing very nested dependencies with their more comfortable conditional distribution which ultimately are “more Gaussian” than the their joint distribution.
Into the LGM framework let us assume to observe <span class="math inline">\(n\)</span> counts, i.e. <span class="math inline">\(\mathbf{y} = y_i = 1,2, \ldots, n\)</span> drawn from Poisson distribution whose mean is <span class="math inline">\(\lambda_i, \forall i \in \mathbf{I}\)</span>. Then a the link function <span class="math inline">\(\mathscr{g}(\cdot)\)</span> is the <span class="math inline">\(\log()\)</span> and relates <span class="math inline">\(\lambda_i\)</span> with the linear predictor and so the latent filed <span class="math inline">\(\theta\)</span>, i.e. <span class="math inline">\(\log(\lambda_i)=\theta_{i}\)</span>. The hyper-parameters are <span class="math inline">\(\boldsymbol\psi = (\tau, \rho)\)</span> with their covariance matrix structure.
<!-- $$ -->
<!-- Q_{\theta}=\tau\left(\begin{array}{cc} -->
<!-- 1 & -\rho \\ -->
<!-- -\rho & 1 -->
<!-- \end{array}\right) -->
<!-- $$ -->
<span class="math display">\[
Q_{\psi}=\boldsymbol\tau\begin{bmatrix}
1 &amp; - \rho &amp; - \rho^{2} &amp; - \rho^{3} &amp; \ldots &amp; - \rho^{n} &amp;  \\
- \rho &amp; 1 &amp; - \rho &amp; - \rho^{2} &amp; - \rho^{3} &amp; - \rho^{n-1} &amp; \\
- \rho^{2} &amp; - \rho &amp; 1 &amp; - \rho &amp; - \rho^{2} &amp; - \rho^{n-2} &amp;  \\
- \rho^{3} &amp; - \rho^{2} &amp; - \rho &amp; 1 &amp; - \rho &amp; - \rho^{n-3} &amp;  \\
- \ldots &amp; - \rho^{3} &amp; - \rho^{2} &amp; - \rho &amp; 1 &amp; - \rho &amp;  \\
- \rho^{n} &amp; - \rho^{n-1} &amp; - \rho^{n-2} &amp; - \rho^{n-3} &amp; - \rho &amp; 1 \\
\end{bmatrix}
\]</span></p>
<p>Let us also to assume once again to model <span class="math inline">\(\boldsymbol\theta\)</span> with an AR(1).
Then fitting the model into the LGM, at first requires to specify an exponential family distribution function, i.e. Poisson on the response <span class="math inline">\(\mathbf{y}\)</span>.
then the <em>higher</em> level (recall last part sec. <a href="inla.html#LGM">4.1</a>) results in:</p>
<p><span class="math display">\[
\pi(\boldsymbol{\mathbf{y}} \mid \boldsymbol{\theta} , \boldsymbol{\psi}) \propto\prod_{i=1}^{\mathbf{I}} \frac{ \exp \left(\theta_{i} y_{i}-e^{\theta_{i}}\right) }{y_{i} !}
\]</span></p>
<p>Then the <em>medium</em> level is for the latent Gaussian Random Field a multivariate gaussian distribution <span class="math inline">\(\boldsymbol{\psi}_{i} \sim \operatorname{MVN}_{2}(\mathbf{0}, \boldsymbol{Q}_{\boldsymbol{\psi}})\)</span>:</p>
<p><span class="math display">\[
\pi(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \propto\left|\boldsymbol{Q}_{\boldsymbol{\psi}}\right|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}_{\boldsymbol{\psi}} \boldsymbol{\theta}\right)
\]</span>
and the <em>lower</em>, where it is specified a joint prior distribution for <span class="math inline">\(\boldsymbol\psi = (\tau, \rho)\)</span>, which is <span class="math inline">\(\pi(\boldsymbol\psi)\)</span>. Following eq.<a href="inla.html#eq:formallgm">(4.5)</a> then:</p>
<p><span class="math display" id="eq:poissonlgm">\[\begin{equation}
\pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \mathbf{y})\propto  \underbrace{\pi(\boldsymbol{\psi})}_{\text {priors}} \times \underbrace{\pi(\boldsymbol\theta \mid \boldsymbol\rho)}_{\text {GMRF}} \times \underbrace{\prod_{i=1}^{\mathbf{I}} \pi\left(\mathbf{y} \mid \boldsymbol\theta, \boldsymbol{\tau}\right)}_{\text {likelihood }}
\tag{4.10}
\end{equation}\]</span></p>
<p>Then recalling the goal for Bayesian Inference, i.e.approximate posterior marginals for <span class="math inline">\(\pi\left(\theta_{i} \mid \mathbf{y}\right)\)</span> and <span class="math inline">\(\pi\left(\tau \mid \boldsymbol{\mathbf{y}}\right)\)</span> and <span class="math inline">\(\pi\left(\rho \mid \boldsymbol{\mathbf{y}}\right)\)</span>. First difficulties regard the fact that Laplace approximations on this model implies the product of a Gaussian distribution and a non-gaussian one. As the INLA key point suggest, the algorithm starts by rearranging the problem so that the “most Gaussian” are computed at first.
Ideally the method can be generally subdivided into three tasks. At first INLA attempts to approximate <span class="math inline">\(\tilde{\pi}(\boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}})\)</span> as the joint posterior of <span class="math inline">\({\pi}(\boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}})\)</span>. Then subsequently will try to approximate <span class="math inline">\(\tilde{\pi}\left(\theta_{i} \mid \boldsymbol\psi, \mathbf{y}\right)\)</span> to their conditional marginal distribution fro <span class="math inline">\(\theta_i\)</span>. In the end explores <span class="math inline">\(\tilde{\pi}(\boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}})\)</span> with numerical methods for integration.
The corresponding integrals to be approximated are:</p>
<ul>
<li>for task 1: <span class="math inline">\(\pi\left(\psi_{k} \mid \boldsymbol{\mathbf{y}}\right)=\int \pi(\boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}}) \mathrm{d} \boldsymbol{\psi}_{-k}\)</span></li>
<li>for task 2: <span class="math inline">\(\pi\left(\theta_{i} \mid \boldsymbol{\mathbf{y}}\right)=\int \pi\left(\theta_{i}, \boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}}\right) \mathrm{d} \boldsymbol{\psi}=\int \pi\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{\mathbf{y}}\right) \pi(\boldsymbol{\psi} \mid \boldsymbol{\mathbf{y}}) \mathrm{d} \boldsymbol{\psi}\)</span></li>
</ul>
<p>As a result the approximations for the marginal posteriors are at first:</p>
<p><span class="math display" id="eq:postapproxprior">\[\begin{equation}
\tilde{\pi}\left(\theta_{j} \mid \boldsymbol{\mathbf{y}}\right)=\int \tilde{\pi}(\boldsymbol{\theta} \mid \boldsymbol{\mathbf{y}}) d \boldsymbol{\theta}_{-j}
\tag{4.11}
\end{equation}\]</span></p>
<p>and then,</p>
<p><span class="math display" id="eq:postapproxlatent">\[\begin{equation}
\tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\mathbf{y}}\right) \approx \sum_{j} \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}^{(j)}, \boldsymbol{\mathbf{y}}\right) \tilde{\pi}\left(\boldsymbol{\psi}^{(j)} \mid \boldsymbol{\mathbf{y}}\right) \Delta_{j}
\tag{4.12}
\end{equation}\]</span></p>
<p>Where in the integral in <a href="inla.html#eq:postapproxlatent">(4.12)</a> <span class="math inline">\(\{\boldsymbol{\psi}^{(j)}\}\)</span> are some relevant integration points and <span class="math inline">\(\{\Delta_j\}\)</span> are weights associated to the set of hyper-parameters ina grid. <span class="citation">(<a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">Michela Blangiardo Marta; Cameletti 2015</a>)</span>.
In other words the bigger the <span class="math inline">\(\Delta_{j}\)</span> weight the more relevant are the integration points. Details on how INLA finds those points is beyond the scope, an indeep resource if offered by <span class="citation"><a href="references.html#ref-wang2018bayesian" role="doc-biblioref">Wang, Yue, and Faraway</a> (<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span> in sec. 2.3.</p>
<!-- ### Approximating $\pi(\boldsymbol{\theta} \mid \boldsymbol{y})$ -->
<!-- provo da qui -->
<!-- INLA is not going to try to estimate the whole joint posterior distribution from expression \@ref(eq:finallgm). Instead it will try to estimate the posterior marginal distribution for each $\theta_{i}$ combination in the latent parameter $\boldsymbol{\theta}$, given the hyper parameter priors specification $\psi_{k}$. Proper estimation methods through Laplace Approximation and search strategies however are beyond the scope of the analysis, further excellent references focused on the task are Rubio -@Bayesian_INLA_Rubio in section 2.2.2, Blangiardo & Cameletti -@Blangiardo-Cameletti in section 4.7.2 and 2.3 in @wang2018bayesian -->
<!-- The goal of Laplace approximation in this context is to approximate marginal posterior integrals each latent parameter $\theta_{i}$ in $\boldsymbol\theta$ in the most efficient and effective way as possible- -->
<!-- \begin{equation} -->
<!--   \pi(\theta_{i} \mid \boldsymbol{y})=\int \pi(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \mathbf{y}) \pi(\boldsymbol{\psi} \mid \mathbf{y}) d \psi -->
<!-- (\#eq:latentparam) -->
<!-- \end{equation} -->
<!-- As well as the marginal posterior distribution for each hyper-parameter $\psi_{k} \in \boldsymbol\psi$, -->
<!-- \begin{equation} -->
<!--   \pi\left(\psi_{k} \mid y\right)=\int \pi(\boldsymbol{\psi} \mid y) d \psi_{-k} -->
<!--   (\#eq:hyperparam) -->
<!-- \end{equation} -->
<!-- <!-- Inla computes the posetrior marginals sfor the poster. the terms inside the integrals are approcimated usign laplaece and nad then thsi integrals can be integrate d numericcally by methods. One you have posterior distribution you can compute quantities of interest like means or quantiles.  -->
<!-- Both of the integrals in \@ref(eq:hyperparam) and \@ref(eq:latentparam) are integrated over the $\boldsymbol\psi$, as a result am approximation of the joint posterior distribution is desired [@Krainski2018]. Following the notation by Rue [-@Rue2009] the integral approximations for hyper-parameters are $\tilde\pi\left(\boldsymbol{\psi} \mid \boldsymbol{y}\right)$ and are plugged in \@ref(eq:latentparam) to obtain the approximation for the posterior marginal of the latent parameter.  -->
<!-- $$ -->
<!-- \tilde{\pi}\left(\theta_{i} \mid y\right) \approx \sum_{j} \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}^{(j)}, y\right) \tilde{\pi}\left(\boldsymbol{\psi}^{(j)} \mid y\right) \Delta_{j} -->
<!-- $$ -->
<!-- where $\Delta_{j}$ are the weights associated with a set of $\psi_{k}$ in a grid [@Krainski2018]. The estimate of $\tilde\pi\left(\boldsymbol{\psi} \mid \boldsymbol{y}\right)$ can be determined in various ways. In order to minimize numerical error @Rue2009 also addresses how this approximation should be. -->
<!-- - step 1: compute the Laplace approximation $\tilde\pi\left(\boldsymbol{\psi} \mid \boldsymbol{y}\right)$  for each hyper parameters marginal: $\tilde\pi\left(\psi_{k} \mid \boldsymbol{y}\right)$ -->
<!-- - step 2: compute Laplace approximation $\tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right)$ marginals for the parameters given the hyper parameter approximation in step 1: $\tilde{\pi}\left(\theta_{i} \mid \boldsymbol{y}\right) \approx \int \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) \underbrace{\tilde{\pi}(\boldsymbol{\psi} \mid \boldsymbol{y})}_{\text {Estim. in step 1 }} \mathrm{d} \psi$ -->
<!-- Then plugging approximation in the integral observed in \@ref(eq:latentparam) it is obtained: -->
<!-- $$ -->
<!-- \tilde{\pi}\left(\theta_{i} \mid y\right) \approx \int \tilde{\pi}\left(\theta_{i} \mid  \boldsymbol{\psi}, y\right) \tilde{\pi}(\boldsymbol{\psi} \mid y) \mathrm{d} \psi -->
<!-- $$ -->
<!-- In the end INLA by its default approximation strategy through  _simplified Laplace approximation_  uses the following numerical approximation to compute marginals:  -->
<!-- $$ -->
<!-- \tilde{\pi}\left(\theta_{i} \mid y\right) \approx \sum_{j} \tilde{\pi}\left(\theta_{i} \mid \boldsymbol{\psi}^{(j)}, y\right) \tilde{\pi}\left(\boldsymbol{\psi}^{(j)} \mid y\right) \Delta_{j} -->
<!-- $$ -->
<!-- where {$\boldsymbol{\psi}^{(j)}$} are a set of values of the hyper param $\psi$ grid used for numerical integration, each of which associated to a specific weight $\Delta_{j}$. The more the weight $\Delta_{j}$ is heavy the more the integration point is relevant. Details on how INLA finds those points is beyond the scope, but the strategy and grids seraches are offered in the appendix follwing both Rubio and Blangiardo. -->
<!-- ### further approximations (prolly do not note include) -->
<!-- INLA focus on this specific integration points by setting up a regular grid about the posterior mode of $\psi$ with CCD (central composite design) centered in the mode [@Bayesian_INLA_Rubio]. -->
<!-- ![CCD to spdetoy dataset, source @Blangiardo-Cameletti](images/CCDapplied.PNG) -->
<!-- The approximation $\tilde{\pi}\left(\theta_{i} \mid y\right)$ can take different forms and be computed in different ways. @Rue2009 also discuss how this approximation should be in order to reduce the numerical error [@Krainski-Rubio]. -->
<!-- Following @Bayesian_INLA_Rubio, approximations of the joint posterior for the hyper paramer $\tilde\pi\left(\psi_{k} \mid \boldsymbol{y}\right)$  is used to compute the marginals for the latent effects and hyper parameters in this way:  -->
<!-- $$ -->
<!-- \left.\tilde{\pi}(\boldsymbol{\psi} \mid \mathbf{y}) \propto \frac{\pi(\boldsymbol{\theta}, \boldsymbol{\psi}, y)}{\tilde{\pi}_{G}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y)}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{*}(\boldsymbol{\psi})} -->
<!-- $$ -->
<!-- In the previous equation $\tilde{\pi}_{G}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y)$ is a gaussian approximation to the full condition of the latent effect ${\theta}^{*}(\boldsymbol{\psi})$ is the mode for a given value of the hyper param vector $\boldsymbol{\psi}$  -->
<!-- At this point there exists three types of approximations for $\pi\left(\boldsymbol{\theta} \mid \boldsymbol{\psi}, y\right)$ -->
<!-- - first with a gaussian approximation, estimating mean $\mu_{i}(\boldsymbol{\psi})$ and variance $\sigma_{i}^{2}(\boldsymbol{\psi})$.  -->
<!-- - second using the _Laplace Approximation._  -->
<!-- - third using _simplified Laplace Approximation_ -->
<!-- (rivedere meglio) -->
<!-- prova questo setting  -->
<!-- ### Hierarchical Bayesian models{#hiermod} -->
<!-- Spatial Models are characterized by many parameters which in turn are tuned by other hyper-parameters. Traditionally Bayesian hierarchical models are not widely adopted since they have high computational burdens, indeed they can handle very complex interactions via random components, especially when dealing with spatio temporal data @Ling.  Blangiardo e Cameletti -@Blangiardo-Cameletti tried to approach the problem from a different angle offering an intuitive solution on how hierarchy relates different levels parameters. This is done by reversing the problem and starting from data back to parameters, instead the other way round. So taking a few steps back the problem can be reformulated by starting from grouping observation into categories and then trying to impose a hierarchical structure on data based on the categories. As a result observations might fall into different categories, underlining their natural characteristics, such as: some of them might belong to category _levels_ like males or females, married or not-married. Moreover diving into the specific problem house prices can be faceted by which floor they belong or whether they are assigned to different energy classes and many others more. As an example Blangiardo and Cameletti example consider grouping data according to just a single 9 _levels_ category. Data for the reasons stated before can be organized such that each single observation (squares in figure below) belongs to its respective mutually exclusive and collectively exhaustive category (circles in figure).   -->
<!-- ![9 levels cat vs observaitions, source @Blangiardo-Cameletti](images/simple.PNG) -->
<!-- Furthermore data can be partitioned into two meta-categories, _fist level_ and _second level_,  highlighting the parameter and hyper paramter chain roles. _First level_ are identified by sampling observations which are drawn by the same probability distribution (squares) . _Second level_ (circles) are categories and might be associated to a set of parameters $\theta=\left\{\theta_{1}, \ldots, \theta_{J}\right\}$. -->
<!-- Since the structure is hierarchical, a DAG (Directed Acyclical Graph) -@Blangiardo-Cameletti representation might sort out ideas. If categories are represented by different $\theta_{j}$ nodes and edges (arrows in the figure) are the logical belonging condition to the category then a single parameter $\theta$ model has the right figure form:  -->
<!-- ![DAG representation of hierarchical structure, source @Blangiardo-Cameletti](images/thetas.PNG)  ![chis, Blangiardo-Cameletti's source](images/chis.PNG) -->
<!-- To fully take into account the hierarchical structure of the data the model should also consider further lelvels. Since $\left\{\theta_{1}, \ldots, \theta_{J}\right\}$ are assumed to come from the same distribution $\pi(\theta_{j})$, then they are also assumed to be sharing information [@Blangiardo-Cameletti], (left figure).  When a further parameter $\boldsymbol{\psi}=\left\{\psi_{1}, \ldots, \psi_{K}\right\}$ is introduced, for which a prior distribution is specified, then the conditional distribution of $\boldsymbol{\theta}$ given $\boldsymbol{\psi}$ is: -->
<!-- $$ -->
<!-- \pi\left(\theta_{1}, \ldots, \theta_{J} \mid \boldsymbol{\psi}\right)=\int \prod_{j=1}^{J} \pi\left(\theta_{j} \mid \psi\right) \pi(\psi) \mathrm{d} \psi -->
<!-- $$ -->
<!-- This is possible thanks to the conditional independence property already encountered in chapter \@ref(inla), which means that each single $\theta$ is conditional independent given $\psi$ -->
<!-- This structure can extended to allow more than two levels of hierarchy since the marginal prior distributions of $\theta$ can be decomposed into the product of their conditional priors distributions given some hyper parameter $\psi$ as well as their prior distribution $\pi(\psi)$. -->
<!-- $$ -->
<!-- \pi(\boldsymbol{\theta})=\int \pi\left(\boldsymbol{\theta} \mid \boldsymbol{\psi}_{1}\right) \pi\left(\boldsymbol{\psi}_{1} \mid \boldsymbol{\psi}_{2}\right) \ldots \pi\left(\boldsymbol{\psi}_{L-1} \mid \boldsymbol{\psi}_{L}\right) \pi\left(\boldsymbol{\psi}_{L}\right) \mathrm{d} \boldsymbol{\psi}_{1} \ldots \mathrm{d} \boldsymbol{\psi}_{L} -->
<!-- $$ -->
<!-- $\boldsymbol{\psi}_{l}$ identifies the hyper pram for the $l_{th}$ level of hierarchy. Each further parameter level $\psi$ is conditioned to its previous in hierarchy level $l-1$ so that the parameter hierarchy chain is respected and all the linear combinations of parameters are carefully evaluated. The *Exchangeability* property enables to have higher $H$ nested DAG (i.e. add further $L$ levels) and to extend the dimensions in which the problem is evaluated, considering also time together with space. From a theoretical point of view there are no constraints to how many $L$ levels can be included in the model, but as a drawback the more the model is nested the more it suffers in terms of interpretability and computational power. Empirical studies have suggest that three levels are the desired amount since they offer a good bias vs variance trade-off. -->
<!-- ## INLA as a Hierarchical Model{#inlahier} -->
<!-- INLA setting presented in section \@ref(approx) can be reorganized following a _Hierarchical structure_ which allows to handle different level parameters in a more compact and coherent way. -->
<!-- Since each of the element of the latent field $\boldsymbol{\theta}$ defined in section \@ref(LGM), which groups all the latent components, is assumed to be similar to each of the other. And since each element comes from a distribution $\pi\left(\theta_{j} \mid \psi\right)$ sampled with the same hyper parameters, $\boldsymbol \psi$. Then there are at least two different levels of the analysis. One lower that regards the $\theta_j$  depending on the one higher the $\boldsymbol\psi$. -->
<!-- The fact that $\theta_j$ are generated by the same distribution authorizes each $\theta_j$ and $\theta_i$ ($i \neq j$) to _exchange_ information, which it is totally different from model settings seen before since they were totally independent. As an example under the frequentist assumption of iid samples the joint prior distribution for $\boldsymbol\theta$ can be rewritten in terms of the product of the each marginal distributions, i.e _likelihood_: -->
<!-- $$ -->
<!-- \pi\left(\theta_{1}, \ldots, \theta_{J}\right)=\prod_{j=1}^{J} \pi\left(\theta_{j}\right)=\pi(\theta)^{J} -->
<!-- $$ -->
<!-- Indeed when a hierarchical structure is imposed on the parameters $\boldsymbol\theta$ each single one is said _Exchangeable_ to the other with respect to the same random generating process. All the $\theta_j$ share the same distribution characterized by the hyper-parameters $\boldsymbol\psi$. -->
<!-- \begin{equation} -->
<!--   \pi\left(\theta_{1}, \ldots, \theta_{J} \mid \psi\right)=\int \prod_{j=1}^{J} \pi\left(\theta_{j} \mid \psi\right) \pi(\psi) \mathrm{d} \psi -->
<!-- (\#eq:exchange) -->
<!-- \end{equation} -->
<!-- One of the major benefits of expressing hierarchy through integral in \@ref(eq:exchange) is the fact that levels can be extended to more than, say 3 (spatio-temporal models [@PACI2017149]). -->
<!-- Then when a Hierarchical structure is imposed on INLA at first, following \@ref(LGM), it is required to specify a probability distribution for $\boldsymbol{y} = \left(y\left(s_{1}\right), \ldots, y\left(s_{n}\right)\right)=\left(y_{1}, \ldots, y_{n}\right)$. A Gaussian distribution for simplicity is chosen. -->
<!-- As a _first level_ parameters are picked up an **exponential family** sampling distribution (i.e. Normally distributed, Gamma one other choice), which is _exchangeable_ with respect to the $\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, f\right\}$ *latent field*  and hyper parameters $\boldsymbol{\psi_{1}}$, which includes also the ones coming from the latent Matérn GP process $w_{i}$. The Spatial Guassian Process is centered in 0 and with Matérn covariance function as $\tau^2$. $w_{i}$ addresses the spatial autocorrelation between observation through a Matérn covariance function $\mathcal{C}(\cdot | \boldsymbol\psi_{1})$ which in turn is tuned by hyper param included in $\boldsymbol{\psi_1}$. Moreover the $w_{i}$ surface has to be passed in the formula method definition \@ref(example) via the `f()` function, so that INLA takes into cosideration the spatial component.  -->
<!-- $$ -->
<!-- \boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}_{1} \sim \mathrm{N}\left(\beta_{0}+ (\mathbf{X}_{i})^{\prime}\boldsymbol{\beta} + w_{i} ,  \tau^2 I_{n}\right)=\prod_{i=1}^{n} \mathrm{N}\left(y_{i} \mid \theta_{i}, \psi_{1}\right) -->
<!-- $$ -->
<!-- Then at the _second level_ the latent field $\boldsymbol{\theta}$ is characterized by a Normal distribution given the remaining hyper parameters $\boldsymbol{\psi}_2$, recall the covariance matrix $\boldsymbol{Q}^{-1}(\boldsymbol{\psi_{2}})$, depending on $\boldsymbol{\psi_{2}}$ hyperparameters, is handled now by a Matérn covariace function depeding on its hyperparamter. This is done in order to map the GP spatial surface into a GMRF by SPDE solutions.   -->
<!-- $$ -->
<!-- \boldsymbol{\theta} \mid \boldsymbol{\psi}_{2} \sim \mathrm{N}\left(\boldsymbol{0}, \mathcal{C}( \cdot , \cdot  \mid \boldsymbol{\psi}_{2})\right) -->
<!-- $$ -->
<!-- In the end hyper parameters $\boldsymbol{\psi}=\left\{\boldsymbol{\psi_{1}}, \boldsymbol{\psi}_{2}\right\}$ having some specified prior distribution i.e. $\boldsymbol{\psi} \sim \pi(\boldsymbol{\psi})$, -->
<!-- ## R-INLA{#rinla} -->
<!-- INLA library and algorithm is developed by the R-INLA project whose package is available on their website at their [source repository](https://www.r-inla.org/download-install). Users can also enjoy on INLA website (newly restyled) a forum where daily discussion group are opened and an active community is keen to answer. Moreover It also contains a number of reference books, among which some of them are fully open sourced. INLA is available for any operating system and and it is built on top of other libraries still not on CRAN.  -->
<!-- The core function of the package is `inla()`and it works as many other regression functions like `glm()`, `lm()` or `gam()`. Inla function takes as argument the _model formula_ i.e. the linear predictor for which it can be specified a number of linear and non-linear effects on covariates as seen in eq. \@ref(eq:linearpredictor), the whole set of available effects are obtained with the command `names(inla.models()$latent)`. Furthermore it requires to specify the dataset and its respective likelihood family, equivalently `names(inla.models()$likelihood)`. -->
<!-- Many other methods in the function can be added through lists, such as `control.family` and `control.fixed` which let the analyst specifying parameter and hyper-paramenters priors family distributions and control hyper parameters. They come in the of nested lists when parameters and hyper paramenters are more than 2, when nothing is specified the default option is non-informativeness. -->
<!-- Inla output objects are inla.dataframe summary-lists-type containing the results from model fitting for which a table is given in figure \@ref(fig:summartable). -->
<!-- ![summary table list object, source: @Krainski-Rubio](images/summarytable.PNG) -->
<!-- <!-- For example following @Blangiardo-Cameletti if the goal is to apply INLA to a normally i.i.d. (identipende and identically distributed) response $\mathbf{y}$ whose conditional distribution given the parameters and hyper parameters is $y_{i} \mid \mu, \sigma^{2} \sim \operatorname{Normal}\left(\mu, \sigma^{2}\right)$. If are specified different prior specification for $\mu$ and $\psi = \frac{1}{\sigma^2}$, formalizing distributions: -->
<!-- <!-- $$ -->
<!-- <!-- \begin{array}{l} -->
<!-- <!-- \mu \sim \operatorname{Normal}\left(\mu_{0}, \sigma_{0}^{2}\right) \\ -->
<!-- <!-- \psi \sim \operatorname{Gamma}(a, b) -->
<!-- <!-- \end{array} -->
<!-- <!-- $$ -->
<!-- <!-- Then the LGM framework that INLA would exploit would start at first by the _higher_ level, i.e. the likelihood: -->
<!-- <!-- $$ -->
<!-- <!-- \boldsymbol{\mathbf{y}} \mid \theta, \psi \sim \prod_{i=1}^{\mathbf{I}} \operatorname{Normal}(\theta, 1 / \psi) -->
<!-- <!-- $$ -->
<!-- <!-- than the _medium_ level for the LGRF:  -->
<!-- For example following @wang2018bayesian if the goal is to apply INLA and to fit an LGM then, assume to have a Poisson distributed response, whose support is $x \in\{0,1,2,3, \ldots\}$ and for _rate_ $\lambda$ parameter $\lambda \in \mathbb{R}^{+}$ and offset $T_i = \{5, 4, 10, 12\}$. Offset accounts the population sample size and gives a convenient way to model relative counts, e.g. per-year, per-state. -->
<!-- The link function $\mathbf{g}(\cdot)$ is exponential, such that the mean structure is linked to the linear predictor by the relation $\lambda_{i}=\exp \left(\eta_{i}\right)$: -->
<!-- $$ -->
<!-- y_{i} \mid \eta_{i} \sim \operatorname{Poisson}\left(T_{i} \lambda_{i}\right) -->
<!-- $$ -->
<!-- Let also suppose as that the latent parameter $\theta =  \{ \eta_{1}, \ldots, \eta_{50} \}$ follows an AR(1) process as in section \@ref(gmrf) with correlation in time $\phi$ -->
<!--        PORV APORV APORVA  PORV A PORV APO R VAPO R VAPORV APORV APORVA PORVPO RVAPO RVAPOR VAPOR VvAA  PORVA             -->
</div>
<div id="rinla" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> R-INLA package</h2>
<p>INLA library and algorithm is developed by the R-INLA project whose package is available on their website at their <a href="https://www.r-inla.org/download-install">source repository</a>. Users can also enjoy on INLA website (recently restyled) a dedicated forum where discussion groups are opened and an active community is keen to answer. Moreover It also contains a number of reference books, among which some of them are fully open sourced. INLA is available for any operating system and it is built on top of other libraries still not on CRAN.
The core function of the package is <code>inla()</code>and it works as many other regression functions like <code>glm()</code>, <code>lm()</code> or <code>gam()</code>. Inla function takes as argument the <em>model formula</em> i.e. the linear predictor for which it can be specified a number of linear and non-linear effects on covariates as seen in eq. <a href="inla.html#eq:linearpredictor">(4.1)</a>, the whole set of available effects are obtained with the command <code>names(inla.models()$latent)</code>. Furthermore it requires to specify the dataset and its respective likelihood family, equivalently <code>names(inla.models()$likelihood)</code>.
Many other methods in the function can be added through lists, such as <code>control.family</code> and <code>control.fixed</code> which let the analyst specifying parameter and hyper-paramenters priors family distributions and control hyper parameters. They come in the of nested lists when parameters and hyper paramenters are more than 2, when nothing is specified the default option is non-informativeness.
Inla output objects are inla.dataframe summary-lists-type containing the results from model fitting for which a table is given in figure <a href="inla.html#fig:summartable">4.3</a>.</p>
<div class="figure"><span id="fig:summartable"></span>
<img src="images/summarytable.PNG" alt="" />
<p class="caption">Figure 4.3: outputs for a <code>inla()</code> call, source: <span class="citation"><a href="references.html#ref-Krainski-Rubio" role="doc-biblioref">E. T. Krainski</a> (<a href="references.html#ref-Krainski-Rubio" role="doc-biblioref">2019</a>)</span></p>
</div>
<p>SPDEtoy dataset <span class="math inline">\(\mathbf{y}\)</span> are two random variables that simulates points location in two coordinates <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:spatplot"></span>
<img src="04-inla_files/figure-html/spatplot-1.png" alt="SPDEtoy bubble plot, author's source" width="672" />
<p class="caption">
Figure 4.4: SPDEtoy bubble plot, author’s source
</p>
</div>
<!-- ![SPDEtoy plot, author's source](images/cotour_toy.png) -->
<p>Imposing an LGM model requires at first to select as a <em>higher</em> hierarchy level a likelihood model for <span class="math inline">\(\mathbf{y}\)</span> i.e. Gaussian (by default), and a model formula (eq. <a href="inla.html#eq:linearpredictor">(4.1)</a>), i.e. <span class="math inline">\(\eta_{i}=\beta_{0}+\beta_{1} s_{1 i}+\beta_{2} s_{2 i}\)</span>, which link function <span class="math inline">\(\mathbf{g}\)</span> is identity. There are not Non-linear effects effect on covariates in $$ nevertheless they can be easily added with <code>f()</code> function. Note that this will allow to integrate random effects i.e. spatial effects inside the model. Secondly in the <em>medium</em> step a LGRF on the latent parameters <span class="math inline">\(\boldsymbol\theta\)</span>. In the <em>lower</em> end some priors distributions <span class="math inline">\(\boldsymbol\psi\)</span> which are Uniform for the intercept indeed Gaussian vagues (default) priors i.e. centered in 0 with very low standard deviation. Furthermore the precision hyper parameter <span class="math inline">\(\tau\)</span> which accounts for the variance of the latent GRF, is set as Gamma distributed with parameters <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\beta = 0.00005\)</span> (default). Note that models are sensitive to prior choices (sec. <a href="prdm.html#priorsspec">5.5</a>), as a consequence if necessary later are revised.
A summary of the model specifications are set below:</p>
<p><span class="math display">\[\begin{equation} 
\begin{split}
y_{i} &amp; \sim N\left(\mu_{i}, \tau^{-1}\right), i=1, \ldots, 200 \\
\mu_{i} &amp;=\beta_{0}+\beta_{1} s_{1 i}+\beta_{2} s_{2 i} \\
\beta_{0} &amp; \sim \text { Uniform } \\
\beta_{j} &amp; \sim N\left(0,0.001^{-1}\right), j=1,2 \\
\tau &amp; \sim G a(1,0.00005)
\end{split}
\end{equation}\]</span></p>
<p>Then the model is fitted within <code>inla()</code> call, specifying the formula, data and the exponential family distribution.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="inla.html#cb10-1" aria-hidden="true" tabindex="-1"></a>formula <span class="ot">&lt;-</span> y <span class="sc">~</span> s1 <span class="sc">+</span> s2</span>
<span id="cb10-2"><a href="inla.html#cb10-2" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">inla</span>(formula, <span class="at">data =</span> SPDEtoy, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span></code></pre></div>
<p>Table <a href="inla.html#tab:tableINLA">4.1</a> offers summary of the posterior marginal values for intercept and covariates’ coefficients, as well as precision. Marginals distributions both for parameters and hyper-parameters can be conveniently plotted as in figure <a href="inla.html#fig:postplot">4.6</a>. From the table it can also be seen that the mean for <span class="math inline">\(s_2\)</span> is negative, so the Norther the y-coordinate, the less is response. That is factual looking at the SPDEtoy contour plot in figure <a href="inla.html#fig:spatplot">4.4</a> where bigger bubbles are concentrated around the origin.</p>
<table class=" lightable-minimal" style="font-family: &quot;Trebuchet MS&quot;, verdana, sans-serif; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:tableINLA">Table 4.1: </span>Summary Posterior quantiles for coefficients
</caption>
<thead>
<tr>
<th style="text-align:left;">
coefficients
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
10.1321487
</td>
<td style="text-align:right;">
0.2422118
</td>
</tr>
<tr>
<td style="text-align:left;">
s1
</td>
<td style="text-align:right;">
0.7624296
</td>
<td style="text-align:right;">
0.4293757
</td>
</tr>
<tr>
<td style="text-align:left;">
s2
</td>
<td style="text-align:right;">
-1.5836768
</td>
<td style="text-align:right;">
0.4293757
</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:postplot"></span>
<img src="04-inla_files/figure-html/postplot-1.png" alt="Linear predictor marginals, plot recoded in `ggplot2`, author's source" width="672" />
<p class="caption">
Figure 4.6: Linear predictor marginals, plot recoded in <code>ggplot2</code>, author’s source
</p>
</div>
<!-- ![Linear predictor marginals, plot recoded in `ggplot2`, author's source](images/marginal_distr.png) -->
<p>In the end R-INLA enables also r-base fashion function to compute statistics on marginal posterior distributions for the density, distribution as well as the quantile function respectively with <code>inla.dmarginal</code>, <code>inla.pmarginal</code> and <code>inla.qmarginal</code>. One option which allows to compute the higher posterior density credibility interval <code>inla.hpdmarginal</code> for a given covariate’s coefficient i.e, <span class="math inline">\(\beta_2\)</span>, such that <span class="math inline">\(\int_{q_{1}}^{q_{2}} \tilde{\pi}\left(\beta_{2} \mid \boldsymbol{y}\right) \mathrm{d} \beta_{2}=0.90\)</span> (90% credibility), whose result is in table below.</p>
<table class=" lightable-minimal" style="font-family: &quot;Trebuchet MS&quot;, verdana, sans-serif; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:higerPosteriorDensityInterval">Table 4.2: </span>Higer Posterior Density Interval for s2 coefficient
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
low
</th>
<th style="text-align:right;">
high
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
level:0.9
</td>
<td style="text-align:right;">
-2.291268
</td>
<td style="text-align:right;">
-0.879445
</td>
</tr>
</tbody>
</table>
<p>Note that the interpretation is more convoluted <span class="citation">(<a href="references.html#ref-wang2018bayesian" role="doc-biblioref">2018</a>)</span> than the traditional frequentist approach: in Bayesian statistics <span class="math inline">\(\beta_{j}\)</span> comes from probability distribution, while frequenstists considers <span class="math inline">\(\beta_{j}\)</span> as fixed unknown quantity whose estimator (random variable conditioned to data) is used to infer the value <span class="citation">(<a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Infrastructure.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prdm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/thesis/edit/master/04-inla.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
