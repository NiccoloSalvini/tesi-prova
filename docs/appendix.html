<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Appendix | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</title>
  <meta name="description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Appendix | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />
  <meta property="og:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Appendix | RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA" />
  
  <meta name="twitter:description" content="This is Niccolò Salvini master’s thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/logo/spatial_logo.png" />

<meta name="author" content="Candidate: Niccolò Salvini" />
<meta name="author" content="Supervisor: PhD Marco L. Della Vedova" />
<meta name="author" content="Assistant Supervisor: PhD Vincenzo Nardelli" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/logo/spatial_logo.png" />
  <link rel="shortcut icon" href="images/logo/spatial_logo.ico" type="image/x-icon" />
<link rel="prev" href="conclusions.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.16/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.3/leaflet.js"></script>
<link href="libs/leaflet-easybutton-1.3.1/easy-button.css" rel="stylesheet" />
<script src="libs/leaflet-easybutton-1.3.1/easy-button.js"></script>
<script src="libs/leaflet-easybutton-1.3.1/EasyButton-binding.js"></script>
<link href="libs/leaflet-locationfilter2-0.1.1/locationfilter.css" rel="stylesheet" />
<script src="libs/leaflet-locationfilter2-0.1.1/locationfilter.js"></script>
<script src="libs/leaflet-locationfilter2-0.1.1/locationfilter-bindings.js"></script>
<link href="libs/ionicons-2.0.1/ionicons.min.css" rel="stylesheet" />
<link href="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.css" rel="stylesheet" />
<script src="libs/leaflet-minimap-3.3.1/Control.MiniMap.min.js"></script>
<script src="libs/leaflet-minimap-3.3.1/Minimap-binding.js"></script>
<script src="libs/core-js-2.5.3/shim.min.js"></script>
<script src="libs/react-16.12.0/react.min.js"></script>
<script src="libs/react-16.12.0/react-dom.min.js"></script>
<script src="libs/reactwidget-1.0.0/react-tools.js"></script>
<script src="libs/reactable-binding-0.2.3/reactable.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>
<script src="js/1book.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logo/spatial_logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i>Colophon</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Web Scraping</a>
<ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#reverse"><i class="fa fa-check"></i><b>2.1</b> A Gentle Introduction on Web Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#anatomy-of-a-url-and-reverse-engineering"><i class="fa fa-check"></i><b>2.2</b> Anatomy of a url and reverse engineering</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="scraping.html"><a href="scraping.html#scraping-with-rvest"><i class="fa fa-check"></i><b>2.2.1</b> Scraping with <code id="ContentArchitecture">rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#ProperScraping"><i class="fa fa-check"></i><b>2.3</b> Searching Technique for Scraping</a></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#best-practices"><i class="fa fa-check"></i><b>2.4</b> Scraping Best Practices and Security provisions</a></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#HTTPmethod"><i class="fa fa-check"></i><b>2.5</b> HTTP overview</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="scraping.html"><a href="scraping.html#spoofing"><i class="fa fa-check"></i><b>2.5.1</b> User Agent and further Identification Headers Spoofing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#possibly"><i class="fa fa-check"></i><b>2.6</b> Dealing with failure</a></li>
<li class="chapter" data-level="2.7" data-path="scraping.html"><a href="scraping.html#parallelscraping"><i class="fa fa-check"></i><b>2.7</b> Parallel Scraping</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="scraping.html"><a href="scraping.html#parallel-furrrfuture"><i class="fa fa-check"></i><b>2.7.1</b> Parallel furrr+future</a></li>
<li class="chapter" data-level="2.7.2" data-path="scraping.html"><a href="scraping.html#parallel-foreachdofuture"><i class="fa fa-check"></i><b>2.7.2</b> Parallel foreach+doFuture</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="scraping.html"><a href="scraping.html#legal"><i class="fa fa-check"></i><b>2.8</b> Legal Profiles</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> API Technology Stack</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#restful-api"><i class="fa fa-check"></i><b>3.1</b> RESTful API</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#plumberapi"><i class="fa fa-check"></i><b>3.1.1</b> Plumber HTTP API</a></li>
<li class="chapter" data-level="3.1.2" data-path="Infrastructure.html"><a href="Infrastructure.html#sanitize"><i class="fa fa-check"></i><b>3.1.2</b> Sanitization</a></li>
<li class="chapter" data-level="3.1.3" data-path="Infrastructure.html"><a href="Infrastructure.html#DoS"><i class="fa fa-check"></i><b>3.1.3</b> Denial Of Service (DoS)</a></li>
<li class="chapter" data-level="3.1.4" data-path="Infrastructure.html"><a href="Infrastructure.html#logging"><i class="fa fa-check"></i><b>3.1.4</b> Logging</a></li>
<li class="chapter" data-level="3.1.5" data-path="Infrastructure.html"><a href="Infrastructure.html#docs"><i class="fa fa-check"></i><b>3.1.5</b> RESTful API docs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker"><i class="fa fa-check"></i><b>3.2</b> Docker</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.1</b> REST-API container</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#nginx"><i class="fa fa-check"></i><b>3.3</b> NGINX reverse Proxy Server and Authorization</a></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-compose"><i class="fa fa-check"></i><b>3.4</b> Docker-Compose</a></li>
<li class="chapter" data-level="3.5" data-path="Infrastructure.html"><a href="Infrastructure.html#HTTPS"><i class="fa fa-check"></i><b>3.5</b> HTTPS(ecure) and SSL certificates</a></li>
<li class="chapter" data-level="3.6" data-path="Infrastructure.html"><a href="Infrastructure.html#aws"><i class="fa fa-check"></i><b>3.6</b> AWS EC2 instance</a></li>
<li class="chapter" data-level="3.7" data-path="Infrastructure.html"><a href="Infrastructure.html#sdwf"><i class="fa fa-check"></i><b>3.7</b> Software CI/CD Workflow</a></li>
<li class="chapter" data-level="3.8" data-path="Infrastructure.html"><a href="Infrastructure.html#further-sf-integrations"><i class="fa fa-check"></i><b>3.8</b> Further SF Integrations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inla.html"><a href="inla.html"><i class="fa fa-check"></i><b>4</b> INLA</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inla.html"><a href="inla.html#LGM"><i class="fa fa-check"></i><b>4.1</b> The class of Latent Gaussian Models (LGM)</a></li>
<li class="chapter" data-level="4.2" data-path="inla.html"><a href="inla.html#gmrf"><i class="fa fa-check"></i><b>4.2</b> Gaussian Markov Random Field (GMRF)</a></li>
<li class="chapter" data-level="4.3" data-path="inla.html"><a href="inla.html#approx"><i class="fa fa-check"></i><b>4.3</b> INLA Laplace Approximations</a></li>
<li class="chapter" data-level="4.4" data-path="inla.html"><a href="inla.html#rinla"><i class="fa fa-check"></i><b>4.4</b> R-INLA package</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prdm.html"><a href="prdm.html"><i class="fa fa-check"></i><b>5</b> Geostatistical Data Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="prdm.html"><a href="prdm.html#GP"><i class="fa fa-check"></i><b>5.1</b> Gaussian Process (GP)</a></li>
<li class="chapter" data-level="5.2" data-path="prdm.html"><a href="prdm.html#spdeapproach"><i class="fa fa-check"></i><b>5.2</b> The Stochastic Partial Differential Equation (SPDE) approach</a></li>
<li class="chapter" data-level="5.3" data-path="prdm.html"><a href="prdm.html#hedonic-rental-price-models"><i class="fa fa-check"></i><b>5.3</b> Hedonic (rental) Price Models</a></li>
<li class="chapter" data-level="5.4" data-path="prdm.html"><a href="prdm.html#criticism"><i class="fa fa-check"></i><b>5.4</b> Model Criticism</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="prdm.html"><a href="prdm.html#predbase"><i class="fa fa-check"></i><b>5.4.1</b> Methods based on the predictive distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="prdm.html"><a href="prdm.html#devbased"><i class="fa fa-check"></i><b>5.4.2</b> Deviance-based Criteria</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="prdm.html"><a href="prdm.html#priorsspec"><i class="fa fa-check"></i><b>5.5</b> Penalized Complexity Priors</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>6</b> Exploratory Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="exploratory.html"><a href="exploratory.html#prep"><i class="fa fa-check"></i><b>6.1</b> Preprocessing and Feature Engineering</a></li>
<li class="chapter" data-level="6.2" data-path="exploratory.html"><a href="exploratory.html#spatassess"><i class="fa fa-check"></i><b>6.2</b> Spatial Dependece Assessement</a></li>
<li class="chapter" data-level="6.3" data-path="exploratory.html"><a href="exploratory.html#factor-counts"><i class="fa fa-check"></i><b>6.3</b> Factor Counts</a></li>
<li class="chapter" data-level="6.4" data-path="exploratory.html"><a href="exploratory.html#mvp"><i class="fa fa-check"></i><b>6.4</b> Assessing the most valuable properties</a></li>
<li class="chapter" data-level="6.5" data-path="exploratory.html"><a href="exploratory.html#assessing-relevant-predictors"><i class="fa fa-check"></i><b>6.5</b> Assessing relevant predictors</a></li>
<li class="chapter" data-level="6.6" data-path="exploratory.html"><a href="exploratory.html#missassimp"><i class="fa fa-check"></i><b>6.6</b> Missing Assessement and Imputation</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="exploratory.html"><a href="exploratory.html#missing-assessement"><i class="fa fa-check"></i><b>6.6.1</b> Missing Assessement</a></li>
<li class="chapter" data-level="6.6.2" data-path="exploratory.html"><a href="exploratory.html#missing-imputation"><i class="fa fa-check"></i><b>6.6.2</b> Missing Imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modelspec.html"><a href="modelspec.html"><i class="fa fa-check"></i><b>7</b> Model Selection &amp; Fitting</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modelspec.html"><a href="modelspec.html#modelspecandmesh"><i class="fa fa-check"></i><b>7.1</b> Model Specification &amp; Mesh Assessement</a></li>
<li class="chapter" data-level="7.2" data-path="modelspec.html"><a href="modelspec.html#spdemodeol"><i class="fa fa-check"></i><b>7.2</b> Building the SPDE object</a></li>
<li class="chapter" data-level="7.3" data-path="modelspec.html"><a href="modelspec.html#model-selection"><i class="fa fa-check"></i><b>7.3</b> Model Selection</a></li>
<li class="chapter" data-level="7.4" data-path="modelspec.html"><a href="modelspec.html#fit"><i class="fa fa-check"></i><b>7.4</b> Parameter Estimation and Results</a></li>
<li class="chapter" data-level="7.5" data-path="modelspec.html"><a href="modelspec.html#plot-gmrf"><i class="fa fa-check"></i><b>7.5</b> Plot GMRF</a></li>
<li class="chapter" data-level="7.6" data-path="modelspec.html"><a href="modelspec.html#spatial-model-criticism"><i class="fa fa-check"></i><b>7.6</b> Spatial Model Criticism</a></li>
<li class="chapter" data-level="7.7" data-path="modelspec.html"><a href="modelspec.html#spatial-prediction-on-a-grid"><i class="fa fa-check"></i><b>7.7</b> Spatial Prediction on a Grid</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>8</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="8.1" data-path="appendix.html"><a href="appendix.html#triangular"><i class="fa fa-check"></i><b>8.1</b> SPDE and Triangulation</a></li>
<li class="chapter" data-level="8.2" data-path="appendix.html"><a href="appendix.html#laplaceapprox"><i class="fa fa-check"></i><b>8.2</b> Laplace Approximation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">RESTful Scraping API for Real Estate data, a Spatial Bayesian modeling perspective with INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendix" class="section level1 unnumbered">
<h1>Appendix</h1>
<!-- ## Gaussian Process{#gpbasics} -->
<!-- Let assume ti have cloud of points represented by two variables $X_1$ and $X_2$, figure \@ref(fig:gpcloud). The cloud of points are observation taken from a realization of two variables e.g. height and weight, What it might be observed is: -->
<!-- ![Left panel: GP cloud of points fitted with a _circle_, Right panel: GP cloud of points fitted with a _ellipse_, source @YT:freitas1](appendix_images/gp_base_1.jpg) -->
<!-- Each circle in both panels figure \@ref(fig:gpclouds) represents a measurements of the observed phenomenon. Let assume to fit a multivariate gaussian distribution to the left panel data. The process of learning is to fit a Gaussian to data, the ultimate goal is to describe the data, the most perfectly evident Gaussian is the one interpolating the points and centering in $\left(0,0\right)$, a circle might be a good guess. Instead for the right one in \@ref(fig:gpclouds), a smart guess could be still centering the mean in, indeed now it is an ellipse describing the variability. -->
<!-- At this point it might be interesting to vectorized what is has been measured, the centers are then compressed into a vector $\mu_{i}$, i.e. it has two components $X_1$ and $X_2$ whose corresponding mean is 0. -->
<!-- $$ -->
<!-- \boldsymbol{\mu}=\left[\begin{array}{ll} -->
<!-- \mu_{x_1} \\ -->
<!-- \mu_{x_2} -->
<!-- \end{array}\right] -->
<!-- $$ -->
<!-- This is true for all the observations which have two coordinates too $x_1$ and $x_2$. for each of the points, e.g. for point 1: -->
<!-- $$ -->
<!-- \mathbf{x_1}=\left[\begin{array}{ll} -->
<!-- x_1 \\ -->
<!-- x_2 -->
<!-- \end{array}\right] -->
<!-- $$ -->
<!-- the can be neagtive positive, the Real numbers, usually we have $\mathbb{R}^{2}$ extending from - infinity to + infinity, to the power of two because we have 2 dimensions, a Real plane. -->
<!-- any point is gaussian distributed when with mean .. an variance.  -->
<!-- how we explain covariance, thorugh _correlation._ -->
<!-- we do it by correlation with its noraml forms. the covariance is the term that goes insisde the matrices in the upper right of the matrxi we have the expectation of $x_1$ times $x_2$, like $\mathbb{E}(x_1 \cdot x_2)$, where the extactation in the gaussian case is the mean which is 0, so the corresponding values is 0. -->
<!-- the covariance essentially is the dot product  [ref dot product](https://mathinsight.org/dot_product_matrix_notation) of $x_1$ and $x_2$ variable, so what happens when you take the dot product of vectors,  -->
<!-- if for example you take a vector that looks like 1 and 0 and you take the dot product of one other vecto 1 and 0, so that: -->
<!-- $$ -->
<!-- \left[\begin{array}{ll} -->
<!-- 1 \\ -->
<!-- 0 -->
<!-- \end{array}\right]\left[\begin{array}{ll} -->
<!-- 1 & 0 \\ -->
<!-- \end{array}\right] = 1 -->
<!-- $$ -->
<!-- You will end up with 1, recall dot productm first element first vcetor times first element second vectro and second element first vector times second element secon vector. So identical vector will get a high dot product value leading to a high similarity measure. Dot product can be indeded as a similarity measure. -->
<!-- ... But if you take two different vector as 1 0 and 0 1 then: -->
<!-- $$ -->
<!-- \left[\begin{array}{ll} -->
<!-- 1 \\ -->
<!-- 0 -->
<!-- \end{array}\right]\left[\begin{array}{ll} -->
<!-- 0 & 1 \\ -->
<!-- \end{array}\right] = 0 -->
<!-- $$ -->
<!-- This time the multiplication leads to 0 value, as a matter of fact they are different. They are no similar. -->
<!-- IF two points are closed the dot product will be high in 2D. What the covariance should be? if variances are assumed to be 1 then in this case i qould expect to be 0, i.e. covariance matrix is: -->
<!-- $$ -->
<!-- \left[\begin{array}{ll} -->
<!-- 1 & 0 \\ -->
<!-- 0 & 1 -->
<!-- \end{array}\right] = \mathbf{cov_{plot1}}(x_1,x_2) -->
<!-- $$ -->
<!-- because I can picka  poin tin two pointa in this cloud. Suppose i increase x1 then my chance of getting a x2 point that is positive or negative is the samee, knowing somthin about x1 give nothign about x2. no information is proivede. On the other hand i the second plot knowing a positive value of 1 can suggest with a certain probability that x2 will be positive (great proabibility. So some information is provided), e.g. -->
<!-- $$ -->
<!-- \left[\begin{array}{ll} -->
<!-- 1   & 0.5 \\ -->
<!-- 0.5 & 1 -->
<!-- \end{array}\right] = \mathbf{cov_{plot2}}(x_1,x_2) -->
<!-- $$ -->
<!-- Some positive number idicates that i expect a positive inc rease iwhen boht of the two are increasing singularly. thsi is what the correlation, the basis to do linear regresssion and non linear- thei is a bivariate gaussian. If the entri3es are  means that they are uncoorellated, if they are non-zero then they are correlated, theby can be both positive or negative (correlatiob) -->
<!-- now lets generate a gassiian distrivution so x_1 and x_2 in 2D and then a third dimension hwere we express probability, this is said joint distribution. So i am going to cu this gaussian at certain point for x_1 and cut a plain rigght thgouth this gauissan imagine to ahava cake and then taka kkniw and cut it.(see the image) -->
<!-- form the man perspective you are goin to see a gaussian distribution, you will be lookong at x_ and you will be seeing a gaussain plot in green. this is the probability of x_1 gievn x_1. also said "conditioned" probabolity. This gaussian has a mean like the one alreasdy seen and this is the center of the gaussian, we can rewrite the mena and variance of the multivariate gaussian describing the cloud of points. sigma are the covaraince martix sigma.  -->
<!-- ... -->
<!-- sigma 1 and signa 2 if you have 1 d varibale the widjth has to be postive, for mulitvariate gaussian equl so here positive definitness: covariance mateix symetric.   -->
<!-- ... -->
<!-- any artibitray variable transposed x time the covarince matrix nedds to be positive.  -->
<!--  what is the mean of this gaussian i might want to know what is the widht of this gaussian would it be great if there is a formula that guven the cloud of point and likelihood estimation. we coilf obtain the red bell in figure.  -->
<!-- Compute the green curve how it is done? this requires some work and it is said matrix *invesrion lemma*, this is foudamental for machine learnign. let's assume it. The theorem says that the the mean fof the gaussian is the mean of x_1 and then some other operation with sigma, see below from paci (miss ref) -->
<!-- ![inverse lemma, Paci source](appendix_images/inverse_lemma.jpg) -->
<!-- the theorem says toi ocnsider a multivariate gaussian a vector 1 and a vector 2 each vecto compinent has a mena and a covarianc matrix, this by lemma gives us the expression and the math behind is no tremendous, but it is long. What it is important is to undestand fto go from a joont to a conditional distribution in our case. thats i the value od the theorem.  -->
<!-- One background further thing: assume that we have a gaussian variable distribution that we want to sample fromm,  we had now ewe are going to do the opposite, before we had poitns and we tried to figure out the curve, now we have the curve and we are gointg to try to rpoduce data. I need to be able to draw sample froma gaussian distribution. i will assume that i have a meachnism that produces a uniform samples, so you have a random numebr generatior with equal probabolity from 0 to 1, I assume a also the cumulative aof a gaussian. -->
<!-- the cumulative of a gaussian is what you get if you syrta summing the area under the curve of the gaussian as you move from the left. value after valure you can plot the cumulative ahead (see figure) the point where there is a flex point is the mean beacuse tha gassias is symmetric. The asymptot is 1 becuase the are under the curve sumes to 1.  -->
<!-- If i can draw a random number form Uniform and the project it to thre cumulative and then finally projct it back to the gaussian distribution. Inverse cumulative mapping. If oyu do this multiple times you are going to have many sample palced next to the mean and as sparse as the variance. in this process of sampling try to sample a point i froma gaussian that has mean 0 a variance 1, now letes try to draw a point from a gaussian with mean mu and variance sigma. ... -->
<!-- In the multivariate case suppose that we have  evctor with two variables how do i draw a vector from a multivariate gasussian with 0 means and plot 1 covarianc ematrix. the theormeem also says that the marginal distribution can be seen by civariance matrix , fist take the men_1 and take upper left element from the covariance matrix obtaining the marginal rpobabiloty for x_1, i.e. -->
<!-- <!-- $$ -->
<!-- <!-- \pi(x_1) = \mathbf{N}(\mu_1, \Sigma_{11}) \\ -->
<!-- <!-- \pi(x_2) = \mathbf{N}(\mu_2, \Sigma_{22})  -->
<!-- <!-- $$ -->
<!-- <!-- then in our problem: -->
<!-- <!-- $$ -->
<!-- <!-- \pi(x_1) = \mathbf{N}(0, 1) \\ -->
<!-- <!-- \pi(x_2) = \mathbf{N}(0, 1)  -->
<!-- <!-- $$ -->
<!-- Then for simplicity we can simplyfy by groupign vector into: -->
<!-- (vectore exoression multivariate) -->
<!-- I need a wau to take square trotto of matrices, if x come sfroma  MVG  -->
<!-- 35:01-- -->
<div id="triangular" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> SPDE and Triangulation</h2>
<p>In order to fit LGM type of models with a spatial component INLA uses SPDE (Stochastic Partial Differential Equations) approach.
Suppose that is it given have a continuous Gaussian random process (a general continuous surface), what SPDE does is to approximate the continuous process by a discrete Gaussian process using a triangulation of the region of the study. Let assume to have a set of points defined by a CRS system (Latitude and Longitude, Easthings and Northings), and let assume that the object of the analysis is to estimate a spatially continuous process. Instead of exploiting this property as a whole, it is estimated the process only at the vertices of this triangulation. This requires to put a Triangulation of the region of the study on top the the process and the software will predict the value of the process at its vertices, then it interpolates the rest of the points obtaining a “scattered” surface.</p>
<div class="figure"><span id="fig:triang"></span>
<img src="appendix_images/appendix_triangularization.jpg" alt="" />
<p class="caption">Figure 8.3: Triangulariation weights and associated process value, <span class="citation"><a href="references.html#ref-YT:paumoraga" role="doc-biblioref">Moraga</a> (<a href="references.html#ref-YT:paumoraga" role="doc-biblioref">2020</a>)</span> source</p>
</div>
<p>Imagine to have a point a location X laying inside a triangle whose vertices are <span class="math inline">\(S_1, S_2 and S_3\)</span>. SPDE operates by setting the values of the process at location x equal to the value of the process at their vertices with some weights, and the weights are given by the <em>Baricentric coordinates (BC)</em>. BC are proportional to the area at the point and the vertices. Let assume to have a piece of Triangulation <span class="math inline">\(\boldsymbol{A}\)</span> and let assume that the goal is to compute the value at location X. X, as in formula above <span class="math inline">\(\boldsymbol{A}\)</span>, would be equal to <span class="math inline">\(S_1\)</span>, multiplied by the area <span class="math inline">\(A_1\)</span> dived by the whole triangle area (<span class="math inline">\(\boldsymbol{A}\)</span>) + <span class="math inline">\(S_2\)</span> multiplied by the area <span class="math inline">\(A_2\)</span> divided by <span class="math inline">\(\boldsymbol{A}\)</span> + <span class="math inline">\(S_3\)</span>, multiplied by the area <span class="math inline">\(A_3\)</span> dived by (<span class="math inline">\(\boldsymbol{A}\)</span>. This would be the value of the process at location X given the triangulations (number of vertices). SPDE is actually approximating the value of the process using a weighted average of the value of the process at the triangle vertices which ir proportional to the area of the below triangle.
In order to do this within INLA <a href="inla.html#inla">4</a> it is needed also a <em>Projection Matrix</em> , figure <a href="appendix.html#fig:projmat">8.4</a>. The Projection matrix maps the continuous GRMF (when it is assumed a GP) from the observation to the triangulation. It essentially assigns the height of the triangle for each vertex of the Triangulation to the process. Matrix <span class="math inline">\(\mathcal{A}\)</span>, whose dimensions are <span class="math inline">\(\mathcal{A_{ng}}\)</span>. It has <span class="math inline">\(n\)</span> rows a <span class="math inline">\(g\)</span> columns, where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(g\)</span> is the number of vertices of the triangulation. Each row has possibly three non-0 values, right matrix in figure <a href="appendix.html#fig:projmat">8.4</a>, and the columns represent the vertices of the triangles that contains the point. Assume to have an observation that coincides with a vertex <span class="math inline">\(S_1\)</span> of the triangle in <a href="appendix.html#fig:triang">8.3</a>, since the point is on top of the vertex (not inside), there are no weights (<span class="math inline">\(A_1 = \mathcal{A}\)</span>) and 1 would be the value at <span class="math inline">\(A_{(1,1)}\)</span> and 0 would be the rest f the values in the row. Now let assume to have an observation coinciding with <span class="math inline">\(S_3\)</span> (vertex in position 3), then the result for <span class="math inline">\(A_{(2,3)}\)</span> would be 1 and the rest 0. Indeed when tha value is X that lies within one of the triangles all the elements of the rows will be 0, but three elements in the row corresponding of the p osition of the vertices <span class="math inline">\(1 = .2, 2 = .2 and g = .6\)</span>, as a result X will be weighted down for the areas.</p>
<div class="figure"><span id="fig:projmat"></span>
<img src="appendix_images/appendix_proj.jpg" alt="" />
<p class="caption">Figure 8.4: Projection Matrix to map values from tringulation back to the GP, <span class="citation"><a href="references.html#ref-YT:paumoraga" role="doc-biblioref">Moraga</a> (<a href="references.html#ref-YT:paumoraga" role="doc-biblioref">2020</a>)</span> surce</p>
</div>
</div>
<div id="laplaceapprox" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Laplace Approximation</h2>
<p><span class="citation"><a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">Michela Blangiardo Marta; Cameletti</a> (<a href="references.html#ref-Blangiardo-Cameletti" role="doc-biblioref">2015</a>)</span> offers an INLA focused intuiton on how the Laplace approximation works for integrals. Let assume that the interest is to compute the follwing integral, assuming the notation followed throughout the analysis:</p>
<p><span class="math display">\[
\int \pi(x) \mathrm{d} x=\int \exp (\log f(x)) \mathrm{d} x
\]</span>
Where <span class="math inline">\(X\)</span> is a random variable for which it is specified a distribution function <span class="math inline">\(\pi\)</span>. Then by the Taylor series expansions <span class="citation">(<a href="references.html#ref-taylorseries" role="doc-biblioref">Fosso-Tande 2008</a>)</span> it is possible to represent the <span class="math inline">\(\log \pi(x)\)</span> evaluated in an exact point <span class="math inline">\(x = x_0\)</span>, so that:</p>
<p><span class="math display" id="eq:secondordexpan">\[\begin{equation}
\log \pi(x) \approx \log \pi\left(x_{0}\right)+\left.\left(x-x_{0}\right) \frac{\partial \log \pi(x)}{\partial x}\right|_{x=x_{0}}+\left.\frac{\left(x-x_{0}\right)^{2}}{2} \frac{\partial^{2} \log \pi(x)}{\partial x^{2}}\right|_{x=x_{0}}
\tag{8.1}
\end{equation}\]</span></p>
<p>Then if it is assumed that <span class="math inline">\(x_0\)</span> is set equal to the mode <span class="math inline">\(x_*\)</span> of the distribution (the highest point), for which <span class="math inline">\(x_{*}=\operatorname{argmax}_{x} \log \pi(x)\)</span>, then the first order derivative with respect to <span class="math inline">\(x\)</span> is 0, i.e. <span class="math inline">\(\left.\frac{\partial \log f(x)}{\partial x}\right|_{x=x_{*}}=0\)</span>. That comes natural since once the function reaches its peak, i.e. the max then the derivative in that point is 0. Then by leaving out the first derivative element in eq. <a href="appendix.html#eq:secondordexpan">(8.1)</a> it is obtained:</p>
<p><span class="math display">\[
\log \pi(x) \approx \log \pi\left(x_{*}\right)+\left.\frac{\left(x-x_{*}\right)^{2}}{2} \frac{\partial^{2} \log \pi(x)}{\partial x^{2}}\right|_{x=x_{*}}
\]</span></p>
<p>Then by integrating what remained, exponantiating and taking out non integrable terms,</p>
<!-- $$ -->
<!-- \int \pi(x) \mathrm{d} x \approx \int \exp \left(\log \pi\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log \pi(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x -->
<!-- $$ -->
<p><span class="math display" id="eq:integraltay">\[\begin{equation}
\int \pi(x) \mathrm{d} x \approx \exp \left(\log \pi\left(x_{*}\right)\right) \int \exp \left(\left.\frac{\left(x-x_{*}\right)^{2}}{2} \frac{\partial^{2} \log \pi(x)}{\partial x^{2}}\right|_{x=x_{*}}\right) \mathrm{d} x
\tag{8.2}
\end{equation}\]</span></p>
<p>At this point it might be already intuited the expression <a href="appendix.html#eq:integraltay">(8.2)</a> is actually the density of a Normal. As a matter of fact, by imposing <span class="math inline">\(\sigma^{2}_{*}=-1 /\left.\frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x_{*}}\)</span>, then expression <a href="appendix.html#eq:integraltay">(8.2)</a> can be rewritten as:</p>
<p><span class="math display" id="eq:rearrangeintr">\[\begin{equation}
\int \pi(x) \mathrm{d} x \approx \exp \left(\log \pi\left(x_{*}\right)\right) \int \exp \left(-\frac{\left(x-x_{*}\right)^{2}}{2 \sigma^{2}_{*}}\right) \mathrm{d} x
\tag{8.3}
\end{equation}\]</span></p>
<p>Furthermore the integrand is the <em>Kernel</em> of the Normal distribution having mean equal to the mode <span class="math inline">\(x_*\)</span> and variance specified as <span class="math inline">\(\sigma^{2}_{*}\)</span>. By computing the finite integral of <a href="appendix.html#eq:rearrangeintr">(8.3)</a> on the closed neighbor <span class="math inline">\([ \alpha, \beta]\)</span> the approximation becomes:</p>
<p><span class="math display">\[
\int_{\alpha}^{\beta} f(x) \mathrm{d} x \approx \pi\left(x_{*}\right) \sqrt{2 \pi \sigma^{2^{*}}}(\Phi(\beta)-\Phi(\alpha))
\]</span>
where <span class="math inline">\(\Phi(\cdot)\)</span> is the cumulative distribution function corresponding value of the Normal disttribution in eq. <a href="appendix.html#eq:rearrangeintr">(8.3)</a>.</p>
<!-- further example: let assume to have a Beta distribution $\operatorname{Beta}(\alpha = 2,8)$ whose parameter are $\alpha$ and  $\beta$ and whose density is:  -->
<p><!-- FORSE L'ESEMPIO --></p>
<p>For example consider the Chi-squared <span class="math inline">\(\chi^{2}\)</span> density function (since it is easily differentiable and Gamma <span class="math inline">\(\Gamma\)</span> term in denominator is constant). The following quantities of interest are the <span class="math inline">\(\log \pi(x)\)</span>, then the <span class="math inline">\(\frac{\partial \log \pi(x)}{\partial x}\)</span>, which has to be set equal to 0 to find the integrating point, and finally <span class="math inline">\(\log \pi(x)\)</span> <span class="math inline">\(\frac{\partial^2 \log \pi(x)}{\partial x^2}\)</span>. The <span class="math inline">\(\chi^{2}\)</span> pdf, whose support is <span class="math inline">\(x \in \mathbb{R}^{+}\)</span> and whose degrees of freedom are <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\chi^{2}(x,k)=\frac{x^{(k / 2-1)} e^{-x / 2}}{2^{k / 2} \,\, \Gamma\left(\frac{k}{2}\right)}, x \geq 0
\]</span></p>
<p>for which are computed:</p>
<p><span class="math display">\[
\log f(x)=(k / 2-1) \log x-x / 2
\]</span>
Ths single variable <em>Score Function</em> and the <span class="math inline">\(x_{*}=\operatorname{argmax}_{x} \log \pi(x)\)</span>,</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned} 
  &amp;\frac{\partial \log \pi(x)}{\partial x}= \frac{(k/2 -1)}{x} - \frac{1}{2} =0 \quad \text {solving for 0 }\\
  &amp;(k/2-1) = x/2\quad \text {moving addends}\\
  &amp;x_* = (k-2) 
\end{aligned}
\end{equation}\]</span></p>
<p>And the <em>Fisher Information</em> in the <span class="math inline">\(x_*\)</span> (for which it is known <span class="math inline">\(\sigma^{2}_*=-1 / f^{\prime \prime}(x)\)</span>)</p>
<p><span class="math display">\[\begin{equation} 
\begin{split}
  &amp;\frac{\partial^2 \log \pi(x)}{\partial x^2}=-\frac{(k / 2-1) }{x^{2}} \quad \text {substituting} \,\,x_* \\
  &amp;= -\frac{(k / 2-1) }{(k-2)^{2}} = - \frac{2(k/2-1)}{2(k-2)^{2}} \quad \text {multiply by 2 } \\
  &amp;=-  \frac{(k-2)}{2(k-2)^{2}} = 2(k-2)  = \sigma^{2}_*\quad \text {change of sign and inverse }
\end{split}
\end{equation}\]</span></p>
<p>finally,</p>
<p><span class="math display">\[
\chi^{2} \stackrel{L A}{\sim} N\left(x_*=k-2, \sigma^{2}_*=2(k-2)\right)
\]</span>
Assuming <span class="math inline">\(k = 8, 16\)</span> degrees of freedom <span class="math inline">\(\chi^{2}\)</span> densities against their Laplace approximation, the figure <a href="appendix.html#fig:laplacevisual">8.5</a> displays how the approximations fits the real density. Integrals are computed in the case of <span class="math inline">\(k = 8\)</span> in the interval <span class="math inline">\((5, 7)\)</span>, leading to a very good Normal approximation that slightly differ from the orginal CHisquared. The same has been done for the <span class="math inline">\(k = 16\)</span> case, whose interval is <span class="math inline">\((12, 17)\)</span> showing other very good approximations. Note that the more are the degrees of freedom the more the chi-squared approximates the Normal leading to better approximations.</p>
<div class="figure" style="text-align: center"><span id="fig:laplacevisual"></span>
<img src="98-appendix_files/figure-html/laplacevisual-1.png" alt="Chisquared density function with parameter $k = 8$ (top) and $k = 16$ (down) solid line. The point line refers to the corresponding Normal approximation obtained using the Laplace method" width="672" />
<p class="caption">
Figure 8.5: Chisquared density function with parameter <span class="math inline">\(k = 8\)</span> (top) and <span class="math inline">\(k = 16\)</span> (down) solid line. The point line refers to the corresponding Normal approximation obtained using the Laplace method
</p>
</div>
<p><span class="math display">\[
\pi(\boldsymbol{\psi}) \quad \boldsymbol{\psi}= \boldsymbol{\psi_1}, \boldsymbol{\psi_2}
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conclusions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/thesis/edit/master/98-appendix.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
